---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2024 - Multilingual Text Detoxification
description: PAN at CLEF 2024 - Multilingual Text Detoxification
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Multilingual Text Detoxification 2024</a></li>
</ul>
</nav>

<main class="uk-section uk-section-default">
    <div class="uk-container">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Multilingual Text Detoxification 2024</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Data</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#submission">Submission</a></li>
<!--                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#results">Results</a></li>-->
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given a toxic piece of text, re-write it in a non-toxic way while saving the main content as much as possible.</li>
                <li>Input: toxic sentences in several languages: English, Russian, Ukrainian, German, Chinese, Amharic, and Arabic.
                </li>
                <li>Output: non-toxic version in the corresponding language.
                </li>
                <li>Evaluation: automatic and manual evaluation based on three parameters: (i) style transfer accuracy; (ii) content preservation; (iii) fluency. <!--[<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/text-detoxification" target="_blank">code</a>]-->
                </li>
                <li>Submission: <a href="#" target="_blank">tira.io</a> as a software submission.
<!--                <li>Submission: <a href="https://www.tira.io/task/pan24-text-detoxification" target="_blank">tira.io</a> preferably as a software submission-->
                </li>
            </ul>

            <h2 id="task">Task</h2>
            <p>
                Identification of toxicity in user texts is an active area of research. Today, social networks such as <a href="https://edition.cnn.com/2021/06/16/tech/facebook-ai-conflict-moderation-groups/index.html">Facebook</a>, <a href="https://about.instagram.com/blog/announcements/introducing-new-tools-to-protect-our-community-from-abuse">Instagram</a> are trying to address the problem of toxicity.
                However, they usually simply block such kinds of texts. 
                We suggest a proactive reaction to toxicity from the user. Namely, we aim at presenting a neutral version of a user message which preserves meaningful content. We denote this task as *detoxification*.
            </p>

            <p>
                Detoxification can be solved with Text Style Transfer (TST) methods.
                Before for detoxification, only unsupervised methods were used. Meaning, there was no parallel corpora, only non-parallel datasets with toxic and non-toxic labels.
                Recenly, the first parallel detoxification corporus (with pairs toxic-neutral text) was introduced for English (<a href="https://aclanthology.org/2022.acl-long.469/">Paradetox</a>) and then for Russian (<a href="https://russe.nlpub.org/2022/tox/">RUSSE-2022</a>). 
                However, the tranfer of detoxification knowledge to a new language is still challenging. 
            </p>

            <p>
                In this competition, we suggest you to create detoxification systems for new languages—Ukrainian, German, Chinese, Amharic, and Arabic—for those there is no parallel detoxification corpus.
                Meaning, you need to solve the task of cross-lingual text detoxification transfer.
            </p>


            <h2 id="data">Data</h2>
            
            <p>    
                ParaDetox datasets for <a href="https://huggingface.co/datasets/s-nlp/paradetox">English</a> and <a href="https://huggingface.co/datasets/s-nlp/ru_paradetox">Russian</a> are already available.
                For each new language, we will provide 1k parallel pairs for development fase and 100 toxic sentences for the test set.
            </p>

            <p>
                <b>Definition of toxiciy</b>. One of the crucial points in this task is to have a common ground on how to estimate if the text is toxic or not.
            </p>

            <h2 id="evaluation">Evaluation</h2>
            <p>
                For the development set, the automatic evaluation metrics set will be available. For the test set, both automatic and manual evaluations will be hold. <b>The final leaderboard</b> will be built based on <b>manual evaluation</b> results.
            </p>

            <p>
                Both automatic and manual evaluation will be based on the main three parameters:
                <ul>
                    <li>
                        <b>Style Transfer Accuracy (STA)</b>: Given the generated paraphrase, the annotators should classify it into one of the classes – toxic or neutral.
                    </li>

                    <li>
                        <b>Content preservation (SIM)</b>: Given two texts (original toxic sentence and generated paraphrase) the annotators should evaluate the similarity of their content and mark them as similar or dissimilar.
                    </li>

                    <li>
                        <b>Fluency task (FL)</b> The output should be no less fluent then the input sentence.
                    </li>
                </ul>
            </p>

            <p>
                To have the one common metric for leaderboard estimation, we will comput <b>J</b> metric as the mean of STA*SIM*FL per sample.
            </p>

            <p>
                For <b>automatic</b> evaluation, the setup for each language will look like follow: (i) STA: binary classifier; (ii) SIM: cosine similarity of LaBSE embeddings; (iii) FL: binary classifier or perplexity.
                All scripts for these metrics calculation will be provided.
            </p>

            <p>
                For <b>manual</b> evaluation, we will create annotation tasks on <a href="https://toloka.ai/">Toloka.ai</a> platform. Then, the final leaderboard will be published on this page.
            </p>
            

            <h2 id="submission">Submission</h2>
            <p>
<!--                All submissions are handled through <a href="https://www.tira.io/task/pan24-text-detoxification" target="_blank">tira.io</a>.-->
                All submissions are handled through <a href="#" target="_blank">tira.io</a>.
                We encourage all participants to use software submissions (via docker) for the sake of reproducibility.
                In a software submission you upload a docker image, which is then built and executed on the test data on tira's servers.
                Please follow tira's extensive documentation (and forums) for instructions.
            </p>

            <h2 id="related-work">Related Work</h2>
            <ul>
                <li>
                    Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev, Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, and Alexander Panchenko. 2022. ParaDetox: Detoxification with Parallel Data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6804–6818, Dublin, Ireland. Association for Computational Linguistics. [<a href="https://aclanthology.org/2022.acl-long.469.pdf">pdf</a>]
                </li>

                <li>
                    Dementieva, Daryna, Daniil Moskovskiy, Varvara Logacheva, David Dale, Olga Kozlova, Nikita Semenov, and Alexander Panchenko. Methods for Detoxification of Texts for the Russian Language Multimodal Technologies and Interaction 5 (2021): no. 9: 54. [<a href="https://www.mdpi.com/2414-4088/5/9/54/pdf">pdf</a>]
                </li>

                <li>
                    David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Semenov, and Alexander Panchenko. 2021. Text Detoxification using Large Pre-trained Neural Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7979–7996, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. [<a href="https://aclanthology.org/2021.emnlp-main.629.pdf">pdf</a>]
                </li>

                <li>
                    Dementieva D. et al. RUSSE-2022: Findings of the First Russian Detoxification Shared Task Based on Parallel Corpora. [<a href="https://www.dialog-21.ru/media/5755/dementievadplusetal105.pdf">pdf</a>]
                </li>
            </ul>

            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
<!--                {% include people-cards/wolska.html %}-->
<!--                Daryna Dementieva - Technical University of Munich, Germany-->
<!--                Alisa Smirnova - Toloka, Switzerland-->
<!--                Dmitry Ustalov - Toloka, Serbia-->
<!--                Alexander Panchenko - Skolkovo Institute of Science and Technology, Russia-->
<!--                Animesh Mukherjee - Indian Institute of Technology Kharagpur, India-->
<!--                Ashaf Elnagar - University of Sharjah, United Arab Emirates-->
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2024 %}
            </div>
        </div>
    </div>
</main>
