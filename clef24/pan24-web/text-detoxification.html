---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2024 - Multilingual Text Detoxification
description: PAN at CLEF 2024 - Multilingual Text Detoxification
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Multilingual Text Detoxification 2024</a></li>
</ul>
</nav>

<main class="uk-section uk-section-default">
    <div class="uk-container">
        <div class="uk-container uk-margin-small">
            <div>
            <h1 class="uk-margin-remove-top">Multilingual Text Detoxification 2024</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#methodology">Methodology</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Data</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#submission">Submission</a></li>
<!--                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#results">Results</a></li>-->
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#timeline">Important Dates</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
            </div>
            <div style="position:absolute;right:50px; top: 300px;">
                <b>Sponsored by</b><br/>
                <a href="https://toloka.ai/" target="_blank">
                    <img src="../pan24-figures/toloka-logo.png" style="width: 200px" alt="Toloka">
                </a>
            </div>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given a toxic piece of text, re-write it in a non-toxic way while saving the main content as much as possible.</li>
                <li>Input: toxic sentences in multiple languages from all over the globe: English, Russian, Ukrainian, German, Spanish, Chinese, Amharic, Arabic, and Hindi.
                </li>
                <li>Output: non-toxic version in the corresponding language.
                </li>
                <li>Evaluation: automatic and manual evaluation based on three parameters: (i) style transfer accuracy; (ii) content preservation; (iii) fluency. <!--[<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/text-detoxification" target="_blank">code</a>]-->
                </li>
                <li>Submission: <a href="#" target="_blank">tira.io</a> as a software submission.
<!--                <li>Submission: <a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/text-detoxification" target="_blank">tira.io</a> preferably as a software submission-->
                </li>
                <li>
                    Main links: [<a href="https://clef2024-labs-registration.dei.unipd.it">CLEF registration</a>] [<a href="https://huggingface.co/textdetox">data</a>] [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/text-detoxification">baselines</a>] [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/text-detoxification">leaderboard</a>] [<a href="https://groups.google.com/g/textdetox-clef2024">google group</a>]
                </li>
            </ul>

            <h2 id="task">Task</h2>
            <p>
                Identification of toxicity in user texts is an active area of research. Today, social networks such as <a href="https://edition.cnn.com/2021/06/16/tech/facebook-ai-conflict-moderation-groups/index.html">Facebook</a>, <a href="https://about.instagram.com/blog/announcements/introducing-new-tools-to-protect-our-community-from-abuse">Instagram</a> are trying to address the problem of toxicity.
                However, they usually simply block such kinds of texts. 
                We suggest a proactive reaction to toxicity from the user. Namely, we aim at presenting a neutral version of a user message which preserves meaningful content. We denote this task as *detoxification*.
            </p>

            <p>
                Detoxification examples in English:
                <table>
                  <tr>
                    <th>Toxic</th>
                    <th>Detoxified</th>
                  </tr>
                  <tr>
                    <td>he had steel b*lls too!</td>
                    <td>->he was brave too!</td>
                  </tr>
                  <tr>
                    <td>delete the page and sh*t up</td>
                    <td>->delete the page</td>
                  </tr>
                  <tr>
                    <td>what a chicken cr*p excuse for a reason.</td>
                    <td>->what a bad excuse for a reason.</td>
                  </tr>
                </table>
            </p>

            <p>
                In this competition, we suggest you to create detoxification systems for 9 languages from several linguitic families: English, Russian, Ukrainian, German, Spanish, Chinese, Amharic, Arabic, and Hindi.
                However, the availability of training corpora will differ between the languages. For English and Russian, the parallel corpora of several thousand of toxic-detoxified pairs (as presented above) are available. So, you can fine-tune text generation models on them.
                For other languages, for the dev phase, no such corpora will be provided. <b>The main challenge</b> of this competition will be to perform an <b>unsupersived and cross-lingual detoxification</b>.
            </p>

            <h2 id="methodology">Methodology</h2>

            <p>
                <b>Unsupervised Methods</b>
            </p>

            <p>
                For the majority of the cases and languages, there is not parallel corpus for the detoxification task. For this reason, we create our competition to recreate the real-life conditions. Some examples of stong unserpvised methods:
                <ul>
                    <li>
                        <b>CondBERT</b> [2,3]: using Masked Language Modeling, mask toxic words in a sentence and rerank the candidates from LM based on their non-toxicity scores.
                    </li>

                    <li>
                        <b>ParaGedi</b> [3]: the detoxification task is viewed as paraphrasing task, but during the generation step toxicity scores of the next token prediction are also taken into account.
                    </li>
                </ul>
            
                The code for this methods can be found <a href="https://github.com/s-nlp/detox/tree/main/emnlp2021/style_transfer">here</a>.
            </p>

            <p>
                <img src="../pan24-figures/detox_condBERT.png" style="width: 400px; display: block; margin: 0 auto;" alt="CondBERT Approach">
            </p>

            <p>
                For the case, where for some language a detoxification corpus or/and model are available (i.e. English), you can use such methods as Backtranslation, Translation of the training corpus to the target language, or Adapter layers.
                Please, refer to [6] for the detailed explanations of the suggested ideas of cross-lingual detoxification knowledge transfer.
            </p>

            <p>
                <b>Supervised Methods</b>
            </p>

            <p>
                If a parallel corpus of toxic-neutral pairs is already available (as, in our case, for English and Russian), then you can fine-tune <i>any</i> text generation model.
                You can refer to the example of ruT5 model for detoxification <a href="https://github.com/s-nlp/russe_detox_2022/tree/main/baselines/t5">example</a> from the previous <a href="https://russe.nlpub.org/2022/tox/">RUSSE-2022</a> competition.
            </p>

            <p>
                <b>Baselines</b>
            </p>

            <p>
                <img src="../pan24-figures/detox_baselines.png" style="width: 600px; display: block; margin: 0 auto;" alt="Unsupervised Baselines">
            </p>
            
            We provide three baslines:
            <ol>
                <li>
                    <b>Duplicate</b>: a simple duplication of the toxic input.
                </li>

                <li>
                    <b>Delete</b>: elimination of a toxic keywords based on a predifined <a href="https://huggingface.co/textdetox">dictionary</a> for each language.
                </li>

                <li>
                    <b>Backtranslation</b>: a more sophisticated cross-lingual transfer method. Translate the input to the language for which powerful detoxification model in available (i.e. English), perform detoxification, and translate back to the target language.
                </li>
            </ol>

            The code for all baselines is available <a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/text-detoxification">here</a>.

            <h2 id="data">Data</h2>
            
            <p>    
                ParaDetox datasets for <a href="https://huggingface.co/datasets/s-nlp/paradetox">English</a> and <a href="https://huggingface.co/datasets/s-nlp/ru_paradetox">Russian</a> are already available. Please, refer to [1] and [4] about more details of the corpora collection.
                **For each** language, we will provide 1k parallel pairs which will be divided into dev and test phases.
            </p>

            <p>
                <b>Definition of toxiciy</b> 
            </p>

            <p>
                One of the crucial points in this task is to have a common ground on how to estimate if the text is toxic or not. 
                In our task, we will work only with <b>explicit</b> types of toxicity—obvious present of obscene and rude lexicon where still there is meaningful neutral content present—and <b>do not</b> work with <b>implicit</b> types—like sarcasm, passive aggressiveness, or direct hate to some group where no neutral content can be found.
                Such implicit toxicity types are challenging to be detoxified so the intent will indeed become non-toxic (i.e. try to detoxify the sentence <i>"I h*te all immigrants, f*ck you all!"</i>).
                For this reason, we tried to pick for our datasets the sentences with explicit toxicity where the main intent is still non-toxic. 
                However, toxicity can be quite a subjective intent. We hope, that we will agree on the majority of the cases what should be toxic or not. In the end, the main goal is to make the texts and the world at least somehow less toxic ;)
            </p>

            <h2 id="evaluation">Evaluation</h2>

            <p>
                <b>Development Phase</b>
            </p>

            <p>
                <img src="../pan24-figures/detox_manual_eval.png" style="width: 400px; display: block; margin: 0 auto;" alt="The concept of text detoxification evaluation">
            </p>
            
            <p>
                For the whole competition, the automatic evaluation metrics set will be available. We provide the multilingual automatic evaluation pipeline based on main three parameters:
                <ul>
                    <li>
                        <b>Style Transfer Accuracy (STA)</b>: Given the generated paraphrase, classify if the style of the new text – toxic or neutral. For this, multilingual toxicity binary <a href="https://huggingface.co/textdetox">classifier</a> will be used.
                    </li>

                    <li>
                        <b>Content preservation (SIM)</b>: Given two texts (original toxic sentence and generated paraphrase), evaluate the similarity of their content. We calculate it as cosine similarity between <a href="https://huggingface.co/sentence-transformers/LaBSE">LaBSe</a> embeddings.
                    </li>

                    <li>
                        <b>Fluency task (FL)</b>: The output should be no less fluent then the input sentence. For both original text and its detoxification, perplexity will be calculated with LM model.
                    </li>
                </ul>
            </p>

            <p>
                To have the one common metric for leaderboard estimation, we will comput <b>J</b> metric as the mean of STA*SIM*FL per sample.
            </p>

            <p>
                All scripts for these metrics calculation will be provided.
            </p>

            <p>
                <b>Test Phase</b>
            </p>

            <p>
                Even if we already have powerful models to classify texts and embed their meanings, the human judgement is still the best for the final decision [5]. So, for the test set, we will perform <b>both</b> manual and automatic evaluation.
                For <b>manual</b> evaluation, we will create annotation tasks on <a href="https://toloka.ai/">Toloka.ai</a> platform corresponding to the same parameters described above. <b>The final leaderboard</b> will be built based on <b>manual evaluation</b> results.
            </p>
            

            <h2 id="submission">Submission</h2>
            <p>
<!--                All submissions are handled through <a href="https://www.tira.io/task/pan24-text-detoxification" target="_blank">tira.io</a>.-->
                All submissions are handled through <a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/text-detoxification" target="_blank">tira.io</a>.
                We encourage all participants to use software submissions (via docker) for the sake of reproducibility.
                In a software submission you upload a docker image, which is then built and executed on the test data on tira's servers.
                Please follow tira's extensive documentation (and forums) for instructions.
            </p>

            <h2 id="timeline">Important Dates</h2>
            <p>
                <ul>
                    <li><b>January, 2024</b>: First data available and run submission opens.</li>
                    <li><b>April 22, 2024</b>: Registration closes.</li>
                    <li><b>May 6, 2024</b>: Run submission deadline and results out.</li>
                    <li><b>May 31, 2024</b>: Participants paper submission.</li>
                    <li><b>July 8, 2023</b>: Camera-ready participant papers submission.</li>
                    <li><b>September 9-12, 2024</b>: CLEF Conference in Grenoble and Touché Workshop.</li>
                </ul>
            </p>

            <h2 id="related-work">Related Work</h2>
            <ol>
                <li>
                    Logacheva V. et. al. <b>ParaDetox: Detoxification with Parallel Data</b>. ACL, 2022. [<a href="https://aclanthology.org/2022.acl-long.469.pdf">pdf</a>]
                </li>

                <li>
                    Dementieva D. et. al. <b>Methods for Detoxification of Texts for the Russian Language</b>. Multimodal Technologies and Interaction 5, 2021. [<a href="https://www.mdpi.com/2414-4088/5/9/54/pdf">pdf</a>]
                </li>

                <li>
                    Dale D. et. al. <b>Text Detoxification using Large Pre-trained Neural Models</b>. EMNLP, 2021. [<a href="https://aclanthology.org/2021.emnlp-main.629.pdf">pdf</a>]
                </li>

                <li>
                    Dementieva D. et al. <b>RUSSE-2022: Findings of the First Russian Detoxification Shared Task Based on Parallel Corpora</b>. Dialogue, 2022. [<a href="https://www.dialog-21.ru/media/5755/dementievadplusetal105.pdf">pdf</a>]
                </li>

                <li>
                    Logacheva, Varvara, et al. <b>A Study on Manual and Automatic Evaluation for Text Style Transfer: The Case of Detoxification.</b> HumEval, 2022. [<a href="https://aclanthology.org/2022.humeval-1.8.pdf">pdf</a>]
                </li>

                <li>
                    Dementieva, D., et al. <b>Exploring Methods for Cross-lingual Text Style Transfer: The Case of Text Detoxification.</b> AACL, 2023. [<a href="http://www.afnlp.org/conferences/ijcnlp2023/proceedings/main-long/cdrom/pdf/2023.ijcnlp-long.70.pdf">pdf</a>]
                </li>
            </ol>

            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/dementieva.html %}
                {% include people-cards/schneider.html %}
                {% include people-cards/wang.html %}
                {% include people-cards/yimam.html %}
                {% include people-cards/elnagar.html %}
                {% include people-cards/ustalov.html %}
                {% include people-cards/stakovskii %}
                {% include people-cards/smirnova.html %}
                {% include people-cards/panchenko.html %}
<!--                Daryna Dementieva - Technical University of Munich, Germany-->
<!--                Alisa Smirnova - Toloka, Switzerland-->
<!--                Dmitry Ustalov - Toloka, Serbia-->
<!--                Alexander Panchenko - Skolkovo Institute of Science and Technology, Russia-->
<!--                Animesh Mukherjee - Indian Institute of Technology Kharagpur, India-->
<!--                Ashaf Elnagar - University of Sharjah, United Arab Emirates-->
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2024 %}
            </div>
        </div>
    </div>
</main>
