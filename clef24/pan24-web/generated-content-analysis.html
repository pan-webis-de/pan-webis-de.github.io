---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2024 - Generative AI Authorship Verification
description: PAN at CLEF 2024 - Generative AI Authorship Verification
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Generative AI Authorship Verification 2024</a></li>
</ul>
</nav>

<main class="uk-section uk-section-default">
    <div class="uk-container">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Voight-Kampff Generative AI Authorship Verification 2024</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Data</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#submission">Submission</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#baselines">Baselines</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#results">Results</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#citation">Citation</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul class="uk-list uk-list-bullet">
                <li>Task: Given two texts, one authored by a human, one by a machine: pick out the human.</li>
                <li>Input: Pairs of texts, one of which was written by a human. [<a href="https://zenodo.org/records/10718757">download</a>]</li>
                <li>Evaluation: ROC-AUC, Brier, C@1, F<sub>1</sub>, F<sub>0.5u</sub> [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/generative-authorship-verification">code</a>]</li>
                <li>Submission: Deployment on TIRA [<a href="https://www.tira.io/task-overview/generative-ai-authorship-verification-panclef-2024/">submit</a>]</li>
                <li>Baselines: PPMd CBC, Authorship Unmasking, Binoculars, DetectLLM, DetectGPT, Fast-DetectGPT, Text length [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/generative-authorship-verification">code</a>]</li>
                <li>Important dates: <strong>13th May 2024</strong> (systems submission; extended, was 6 May), <strong>31 May 2024</strong> (notebook paper submission) [<a href="https://clef2024.imag.fr/index.php?page=Pages/schedule.html">CLEF'24 lab schedule</a>]</li>
            </ul>


            <h2 id="task">Task</h2>
            <p>With Large Language Models (LLMs) improving at breakneck speed and seeing more widespread adoption every day, it is getting increasingly hard to discern whether a given text was authored by a human being or a machine. Many classification approaches have been devised to help humans distinguish between human- and machine-authored text, though often without questioning the fundamental and inherent feasibility of the task itself.</p>

            <p>With years of experience in a related but much broader field—authorship verification—, we set out to answer whether this task can be solved. We start with the simplest arrangement of a suitable task setup: <strong>Given two texts, one authored by a human, one by a machine: pick out the human.</strong></p>

            <p>The <em>Generative AI Authorship Verification Task</em> @ PAN is organized in collaboration with the <em>Voight-Kampff Task</em> @ <a href="https://eloquent-lab.github.io/">ELOQUENT Lab</a> in a builder-breaker style. PAN participants will build systems to tell human and machine apart, while ELOQUENT participants will investigate novel text generation and obfuscation methods for avoiding detection.</p>

            <h2 id="data">Data</h2>

            <p>Test data for this task will be compiled from the submissions of ELOQUENT participants and will comprise multiple text genres, such as news articles, Wikipedia intro texts, or fanfiction. Additionally, PAN participants will be provided with a bootstrap dataset of real and fake news articles spanning multiple 2021 U.S. news headlines. The bootstrap dataset can be used for training purposes, though we strongly recommend using other sources as well.</p>

            <p><strong>Download instructions:</strong> The dataset is available <a href="https://zenodo.org/records/10718757">via Zenodo</a>. Please register first at <a href="https://www.tira.io/task-overview/generative-ai-authorship-verification-panclef-2024/">Tira</a> and then request access on Zenodo using the same email address. The dataset contains copyrighted material and may be used only for research purposes. No redistribution allowed.</p>

            <p>The <strong>bootstrap dataset</strong> is provided as a set of newline-delimited JSON files. Each file contains a list of articles, written either by (any number of) human authors or a single machine. That is, the file <code>human.jsonl</code> contains only human texts, whereas a file <code>gemini-pro.jsonl</code> contains articles about the same topics, but written entirely by Google's Gemini Pro. The file format is as follows:</p>

            <pre class="prettyprint"><code class="lang-json">{"id": "gemini-pro/news-2021-01-01-2021-12-31-kabulairportattack/art-081", "text": "..."}
{"id": "gemini-pro/news-2021-01-01-2021-12-31-capitolriot/art-050", "text": "..."}
...</code></pre>

            <p>The article IDs and line orderings are the same across all files (except for the model-specific prefix before the first <code>/</code>), so the same line always corresponds to the same topic, but from different “authors”.</p>

            <p>The <strong>test dataset</strong> will be provided in a different format. Instead of individual files, only a single JSONL file will be given, each line containing a pair of texts:</p>

            <pre class="prettyprint"><code class="lang-json">{"id": "iixcWBmKWQqLAwVXxXGBGg", "text1": "...", "text2": "..."}
{"id": "y12zUebGVHSN9yiL8oRZ8Q", "text1": "...", "text2": "..."}
...</code></pre>

            <p>The IDs will be scrambled and the participant's task is to generate an appropriate output file with predictions for which of the two texts is the human one (see <a href="#submission">Submission</a> below).</p>

            <h2 id="evaluation">Evaluation</h2>
            Systems will be evaluated with the same measures as previous installments of the PAN authorship verification tasks. The following metrics will be used:</p>

            <ul class="uk-list uk-list-bullet">
                <li>ROC-AUC: The area under the ROC (Receiver Operating Characteristic) curve.</li>
                <li>Brier: The complement of the Brier score (mean squared loss).</li>
                <li>C@1: A modified accuracy score that assigns non-answers (score = 0.5) the average accuracy of the remaining cases.</li>
                <li>F<sub>1</sub>: The harmonic mean of precision and recall.</li>
                <li>F<sub>0.5u</sub>: A modified F<sub>0.5</sub> measure (precision-weighted F measure) that treats non-answers (score = 0.5) as false negatives.</li>
                <li>The arithmetic mean of all the metrics above.</li>
            </ul>

            <p>The evaluator for the task will output the above measures as JSON like so:</p>
            <pre class="prettyprint"><code class="lang-json">{
    "roc-auc": 0.992,
    "brier": 0.979,
    "c@1": 0.978,
    "f1": 0.978,
    "f05u": 0.978,
    "mean": 0.981
}</code></pre>


            <h2 id="submission">Submission</h2>
            <p>Participants will submit their systems as Docker images through the <a href="https://www.tira.io/task-overview/generative-ai-authorship-verification-panclef-2024/">Tira</a> platform. It is not expected that submitted systems are actually <em>trained</em> on Tira, but they must be standalone and runnable on the platform without requiring contact to the outside world (evaluation runs will be sandboxed).</p>

            <p>The submitted software must be executable inside the container via a command line call. The script must take two arguments: an input file (an absolute path to the input JSONL file) and an output directory (an absolute path to where the results will be written):</p>

            <p>Within Tira, the input file will be called <code>dataset.jsonl</code>, so with the pre-defined Tira placeholders, your software should be invoked like this:</p>
            <pre class="prettyprint"><code class="lang-console">$ mySoftware $inputDataset/dataset.jsonl $outputDir</code></pre>

            <p>Within <code>$outputDir</code>, a single (!) file with the file extension <code>*.jsonl</code> must be created with the following format:</p>

            <pre class="prettyprint"><code class="lang-json">{"id": "iixcWBmKWQqLAwVXxXGBGg", "is_human": 1.0}
{"id": "y12zUebGVHSN9yiL8oRZ8Q", "is_human": 0.3}
...</code></pre>

            <p>For each test case in the input file, an output line must be written with the ID of the input text pair and a confidence score between <code>0.0</code> and <code>1.0</code>. A score <code>&lt; 0.5</code> means that <code>text1</code> is believed to be human-authored. A score <code>&gt; 0.5</code> means that <code>text2</code> is believed to be human-authored. A score of <em>exactly</em> <code>0.5</code> means the case is undecidable. Participants are encouraged to answer with <code>0.5</code> rather than making a <em>wrong</em> prediction.</p>

            <p><strong>All test cases must be processed in isolation without information leakage between them!</strong> Even though systems may be given an input file with multiple JSON lines at once for reasons of efficiency, these inputs must be processed and answered just the same as if only a single line were given. Answers for any one test case must not depend on other cases in the input dataset!</p>

            <p><strong>Tip:</strong> You can test your submission using the <code>pan24-generative-authorship-smoke-test</code> dataset and evaluator (which is different from the final test dataset).</p>

            <h2 id="baselines">Baselines</h2>
            <p>We provide six (seven) LLM detection baselines as re-implementations from the original papers:</p>
            <ul class="uk-list uk-list-bullet">
                <li>
                    PPMd Compression-based Cosine
                    [<a href="https://ieeexplore.ieee.org/abstract/document/1607268">Sculley and Brodley, 2006</a>]
                    [<a href="https://dl.acm.org/doi/abs/10.1145/3098954.3104050">Halvani et al., 2017</a>]
                </li><li>
                    Authorship Unmasking
                    [<a href="https://dl.acm.org/doi/abs/10.1145/1015330.1015448">Koppel and Schler, 2004</a>]
                    [<a href="https://aclanthology.org/N19-1068/">Bevendorff et al., 2019</a>]
                </li><li>
                    Binoculars
                    [<a href="https://arxiv.org/abs/2401.12070">Hans et al., 2024</a>]
                </li><li>
                    DetectLLM LRR and NPR
                    [<a href="https://arxiv.org/abs/2306.05540">Su et al., 2023</a>]
                </li><li>
                    DetectGPT
                    [<a href="https://arxiv.org/abs/2301.11305">Mitchell et al., 2023</a>]
                </li><li>
                    Fast-DetectGPT
                    [<a href="https://arxiv.org/abs/2310.05130">Bao et al., 2023</a>]
                </li><li>
                    Text length
                </li>
            </ul>

            <p>With PPMd CBC and Authorship unmasking, we provide two bag-of-words authorship verification models. Binoculars, DetectLLM, and DetectGPT use large language models to measure text perplexity. The text length baseline serves mainly as a data sanity check and is designed to have random performance.</p>

            <p>The baselines are published on <a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/generative-authorship-verification">GitHub</a>. You can run them locally, in a Docker container or using <code>tira-run</code>. All baselines come with a CLI and usage instructions. Their general usage is:</p>

            <pre class="prettyprint"><code class="console">$ baseline BASELINENAME [OPTIONS] INPUT_FILE OUTPUT_DIRECTORY</code></pre>

            Use <code>--help</code> on any subcommand for more information:

            <pre class="prettyprint"><code class="console">$ baseline --help
Usage: baseline [OPTIONS] COMMAND [ARGS]...

  PAN'24 Generative Authorship Detection baselines.

Options:
  --help  Show this message and exit.

Commands:
  binoculars     PAN'24 baseline: Binoculars.
  detectgpt      PAN'24 baseline: DetectGPT.
  detectllm      PAN'24 baseline: DetectLLM.
  fastdetectgpt  PAN'24 baseline: Fast-DetectGPT.
  length         PAN'24 baseline: Text length.
  ppmd           PAN'24 baseline: Compression-based cosine.
  unmasking      PAN'24 baseline: Authorship unmasking.</code></pre>

            <p>More information on how to install and run the baselines can be found in the <a href="https://github.com/pan-webis-de/pan-code/tree/master/clef24/generative-authorship-verification#readme">README on GitHub</a>.</p>


            <h2 id="results">Results</h2>
            <p>In the following are the final scores of all teams participating in the 2024 shared task. The individual effectiveness scores are aggregates across all test datasets corrected by half a standard deviation to penalize unstable classification performance. The ranking is based on the mean of all individual scores (last column).</p>

            <p>If teams submitted multiple systems, only the system performing best on the main test dataset is included in the ranking. Scores marked with * are only approximate, since the system failed to run on one or more datasets and the missing scores were filled with the mean score of all other systems.</p>

            <div class="uk-container uk-margin-medium">
                <table class="uk-table uk-table-divider uk-table-small">
                    <thead>
                    <tr>
                        <th>Team</th>
                        <th>System</th>
                        <th>ROC-AUC</th>
                        <th>Brier</th>
                        <th>C@1</th>
                        <th>F<sub>1</sub></th>
                        <th>F<sub>0.5u</sub></th>
                        <th>Mean <span data-uk-icon="arrow-down"></span></th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>marsan</td>
                        <td>staff-trunk</td>
                        <td>0.961</td>
                        <td>0.928</td>
                        <td>0.912</td>
                        <td>0.884</td>
                        <td>0.932</td>
                        <td>0.924</td>
                    </tr>
                    <tr>
                        <td>you-shun-you-de</td>
                        <td>charitable-mole_v3</td>
                        <td>0.931</td>
                        <td>0.926</td>
                        <td>0.928</td>
                        <td>0.905</td>
                        <td>0.913</td>
                        <td>0.921</td>
                    </tr>
                    <tr>
                        <td>baselineavengers</td>
                        <td>svm</td>
                        <td>0.925</td>
                        <td>0.869</td>
                        <td>0.882</td>
                        <td>0.875</td>
                        <td>0.869</td>
                        <td>0.886</td>
                    </tr>
                    <tr>
                        <td>g-fosunlpteam</td>
                        <td>gritty-producer</td>
                        <td>0.889</td>
                        <td>0.875</td>
                        <td>0.887</td>
                        <td>0.884</td>
                        <td>0.884</td>
                        <td>0.884</td>
                    </tr>
                    <tr>
                        <td>fosu-stu</td>
                        <td>merciless-broth</td>
                        <td>0.895</td>
                        <td>0.881</td>
                        <td>0.860</td>
                        <td>0.815</td>
                        <td>0.830</td>
                        <td>0.858</td>
                    </tr>
                    <tr>
                        <td>lam</td>
                        <td>blistering-moss</td>
                        <td>0.851</td>
                        <td>0.850</td>
                        <td>0.850</td>
                        <td>0.852</td>
                        <td>0.849</td>
                        <td>0.851</td>
                    </tr>
                    <tr>
                        <td>drocks</td>
                        <td>muffled-stock</td>
                        <td>0.866</td>
                        <td>0.863</td>
                        <td>0.834</td>
                        <td>0.825</td>
                        <td>0.820</td>
                        <td>0.843</td>
                    </tr>
                    <tr>
                        <td>aida</td>
                        <td>corporate-burn</td>
                        <td>0.831</td>
                        <td>0.825</td>
                        <td>0.795</td>
                        <td>0.788</td>
                        <td>0.782</td>
                        <td>0.806</td>
                    </tr>
                    <tr>
                        <td>cnlp-nits-pp</td>
                        <td>direct-velocity</td>
                        <td>0.844</td>
                        <td>0.793</td>
                        <td>0.805</td>
                        <td>0.789</td>
                        <td>0.792</td>
                        <td>0.806</td>
                    </tr>
                    <tr>
                        <td>ap-team</td>
                        <td>marinated-pantone</td>
                        <td>0.853</td>
                        <td>0.862</td>
                        <td>0.795</td>
                        <td>0.718</td>
                        <td>0.742</td>
                        <td>0.796</td>
                    </tr>
                    <tr>
                        <td>heartsteel</td>
                        <td>canary-paint</td>
                        <td>0.777</td>
                        <td>0.777</td>
                        <td>0.777</td>
                        <td>0.780</td>
                        <td>0.777</td>
                        <td>0.778</td>
                    </tr>
                    <tr>
                        <td>verification-team</td>
                        <td>merciless-lease</td>
                        <td>0.799</td>
                        <td>0.788</td>
                        <td>0.740</td>
                        <td>0.740</td>
                        <td>0.741</td>
                        <td>0.763</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline Binoculars (Falcon-7B)</td>
                        <td>0.751</td>
                        <td>0.780</td>
                        <td>0.734</td>
                        <td>0.720</td>
                        <td>0.720</td>
                        <td>0.741</td>
                    </tr>
                    <tr>
                        <td>huangbaijian</td>
                        <td>bitter-metaphor</td>
                        <td>0.760*</td>
                        <td>0.785*</td>
                        <td>0.727*</td>
                        <td>0.706*</td>
                        <td>0.703*</td>
                        <td>0.737*</td>
                    </tr>
                    <tr>
                        <td>no-999</td>
                        <td>method2</td>
                        <td>0.901</td>
                        <td>0.758</td>
                        <td>0.733</td>
                        <td>0.549</td>
                        <td>0.653</td>
                        <td>0.722</td>
                    </tr>
                    <tr>
                        <td>iimasnlp</td>
                        <td>final-run4-gnnllm_llmft_stylofeat-partitionB</td>
                        <td>0.723*</td>
                        <td>0.746*</td>
                        <td>0.695*</td>
                        <td>0.690*</td>
                        <td>0.676*</td>
                        <td>0.707*</td>
                    </tr>
                    <tr>
                        <td>j1j</td>
                        <td>product-dust</td>
                        <td>0.692</td>
                        <td>0.678</td>
                        <td>0.678</td>
                        <td>0.732</td>
                        <td>0.680</td>
                        <td>0.694</td>
                    </tr>
                    <tr>
                        <td>jaha</td>
                        <td>greasy-chest</td>
                        <td>0.736</td>
                        <td>0.731</td>
                        <td>0.731</td>
                        <td>0.590</td>
                        <td>0.614</td>
                        <td>0.683</td>
                    </tr>
                    <tr>
                        <td>qinnnruiii</td>
                        <td>tender-couple</td>
                        <td>0.692*</td>
                        <td>0.732*</td>
                        <td>0.673*</td>
                        <td>0.652*</td>
                        <td>0.652*</td>
                        <td>0.681*</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline Binoculars (Mistral-7B)</td>
                        <td>0.676</td>
                        <td>0.711</td>
                        <td>0.663</td>
                        <td>0.654</td>
                        <td>0.648</td>
                        <td>0.671</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline DetectLLM-LRR (Mistral-7B)</td>
                        <td>0.656</td>
                        <td>0.758</td>
                        <td>0.617</td>
                        <td>0.618</td>
                        <td>0.618</td>
                        <td>0.654</td>
                    </tr>
                    <tr>
                        <td>petropoulossiblings</td>
                        <td>extended-parakeet</td>
                        <td>0.594</td>
                        <td>0.694</td>
                        <td>0.670</td>
                        <td>0.631</td>
                        <td>0.590</td>
                        <td>0.641</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline Fast-DetectGPT (Mistral-7B)</td>
                        <td>0.637</td>
                        <td>0.710</td>
                        <td>0.616</td>
                        <td>0.611</td>
                        <td>0.608</td>
                        <td>0.638</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline DetectLLM-NPR (Mistral-7B)</td>
                        <td>0.635</td>
                        <td>0.701</td>
                        <td>0.598</td>
                        <td>0.597</td>
                        <td>0.599</td>
                        <td>0.626</td>
                    </tr>
                    <tr>
                        <td>turtlewu</td>
                        <td>0.981_smoke_transofrmer</td>
                        <td>0.645</td>
                        <td>0.649</td>
                        <td>0.587</td>
                        <td>0.578</td>
                        <td>0.577</td>
                        <td>0.608</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline Text Length</td>
                        <td>0.608</td>
                        <td>0.607</td>
                        <td>0.607</td>
                        <td>0.596</td>
                        <td>0.596</td>
                        <td>0.604</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline DetectLLM-NPR (Falcon-7B)</td>
                        <td>0.584</td>
                        <td>0.680</td>
                        <td>0.559</td>
                        <td>0.551</td>
                        <td>0.550</td>
                        <td>0.585</td>
                    </tr>
                    <tr>
                        <td>gra</td>
                        <td>ash-causeway</td>
                        <td>0.500</td>
                        <td>0.750</td>
                        <td>0.467</td>
                        <td>0.634</td>
                        <td>0.521</td>
                        <td>0.574</td>
                    </tr>
                    <tr>
                        <td>you-na-you-de</td>
                        <td>lzj_v2</td>
                        <td>0.593</td>
                        <td>0.598</td>
                        <td>0.598</td>
                        <td>0.458</td>
                        <td>0.565</td>
                        <td>0.565</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline DetectGPT (Mistral-7B)</td>
                        <td>0.550</td>
                        <td>0.671</td>
                        <td>0.532</td>
                        <td>0.533</td>
                        <td>0.532</td>
                        <td>0.564</td>
                    </tr>
                    <tr>
                        <td>team-chenteam-chen123hhh</td>
                        <td>beige-limit</td>
                        <td>0.627</td>
                        <td>0.660</td>
                        <td>0.590</td>
                        <td>0.442</td>
                        <td>0.433</td>
                        <td>0.555</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline PPMd</td>
                        <td>0.555</td>
                        <td>0.622</td>
                        <td>0.523</td>
                        <td>0.508</td>
                        <td>0.507</td>
                        <td>0.544</td>
                    </tr>
                    <tr>
                        <td>ai-team</td>
                        <td>ash-ricotta</td>
                        <td>0.525</td>
                        <td>0.622</td>
                        <td>0.506</td>
                        <td>0.499</td>
                        <td>0.498</td>
                        <td>0.531</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline DetectGPT (Falcon-7B)</td>
                        <td>0.493</td>
                        <td>0.663</td>
                        <td>0.489</td>
                        <td>0.487</td>
                        <td>0.487</td>
                        <td>0.525</td>
                    </tr>
                    <tr>
                        <td>younanyousha</td>
                        <td>brownian-architect</td>
                        <td>0.598</td>
                        <td>0.604</td>
                        <td>0.604</td>
                        <td>0.318</td>
                        <td>0.378</td>
                        <td>0.504</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline Fast-DetectGPT (Falcon-7B)</td>
                        <td>0.480</td>
                        <td>0.626</td>
                        <td>0.474</td>
                        <td>0.457</td>
                        <td>0.458</td>
                        <td>0.500</td>
                    </tr>
                    <tr>
                        <td>foshan-university-of-guangdong</td>
                        <td>adjacent-rate</td>
                        <td>0.464</td>
                        <td>0.660</td>
                        <td>0.462</td>
                        <td>0.448</td>
                        <td>0.448</td>
                        <td>0.497</td>
                    </tr>
                    <tr>
                        <td>e-comm-tech</td>
                        <td>great-plan</td>
                        <td>0.463</td>
                        <td>0.651</td>
                        <td>0.467</td>
                        <td>0.445</td>
                        <td>0.446</td>
                        <td>0.497</td>
                    </tr>
                    <tr>
                        <td>yomiya</td>
                        <td>current-boutique</td>
                        <td>0.645</td>
                        <td>0.798</td>
                        <td>0.325</td>
                        <td>0.307</td>
                        <td>0.323</td>
                        <td>0.480</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline Unmasking</td>
                        <td>0.586</td>
                        <td>0.749</td>
                        <td>0.337</td>
                        <td>0.323</td>
                        <td>0.328</td>
                        <td>0.467</td>
                    </tr>
                    <tr>
                        <td>karami-kheiri</td>
                        <td>bare-broker</td>
                        <td>0.627</td>
                        <td>0.789</td>
                        <td>0.304</td>
                        <td>0.282</td>
                        <td>0.296</td>
                        <td>0.460</td>
                    </tr>
                    <tr class="uk-text-italic uk-background-muted">
                        <td colspan="2">Baseline DetectLLM-LRR (Falcon-7B)</td>
                        <td>0.441</td>
                        <td>0.600</td>
                        <td>0.428</td>
                        <td>0.413</td>
                        <td>0.413</td>
                        <td>0.460</td>
                    </tr>
                    <tr>
                        <td>lm-detector</td>
                        <td>detector</td>
                        <td>0.493</td>
                        <td>0.586</td>
                        <td>0.409</td>
                        <td>0.366</td>
                        <td>0.382</td>
                        <td>0.450</td>
                    </tr>
                    </tbody>
                </table>
            </div>

            <h2 id="citation">Citation</h2>
            <p>To cite the <em>Voight-Kampff Generative AI Authorship Verification Task @ PAN'24</em>, please use the following BibTeX entry:</p>
            <pre><code class="lang-bibtex">{% raw %}@InProceedings{bevendorff:2024,
  author =                   {Janek Bevendorff and Matti Wiegmann and Jussi Karlgren and Luise D{\"u}rlich and Evangelia Gogoulou and Aarne Talman and Efstathios Stamatatos and Martin Potthast and Benno Stein},
  booktitle =                {Working Notes of CLEF 2024 - Conference and Labs of the Evaluation Forum},
  editor =                   {Guglielmo Faggioli and Nicola Ferro and Petra Galu\v{s}\v{c}{\'a}kov{\'a} Alba Garc{\'\i}a Seco de Herrera},
  month =                    sep,
  series =                   {CEUR Workshop Proceedings},
  site =                     {Grenoble, France},
  title =                    {{Overview of the ``Voight-Kampff'' Generative AI Authorship Verification Task at PAN and ELOQUENT 2024}},
  year =                     2024
}{% endraw %}</code></pre>

            <h2 id="related-work">Related Work</h2>
            <ul class="uk-list uk-list-bullet">
				<li>
                    Bevendorff, Janek, Xavier Bonet Casals, Berta Chulvi, Daryna Dementieva, Ashaf Elnagar, Dayne Freitag, Maik Fröbe, et al. 2024. <a href="https://webis.de/publications.html#bevendorff_2024b">Overview of PAN 2024: Multi-Author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification: Extended Abstract.</a> In Lecture Notes in Computer Science, 3–10. Lecture Notes in Computer Science. Cham: Springer Nature Switzerland.
                </li>
                <li>
                    Uchendu, Adaku, Thai Le, Kai Shu, and Dongwon Lee. 2020. <a href="https://aclanthology.org/2020.emnlp-main.673/">Authorship Attribution for Neural Text Generation.</a> In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 8384–95. Online: Association for Computational Linguistics.
                </li>
                <li>
                    Jakesch, Maurice, Jeffrey T. Hancock, and Mor Naaman. 2023. <a href="https://www.pnas.org/doi/10.1073/pnas.2208839120">Human Heuristics for AI-Generated Language Are Flawed.</a> Proceedings of the National Academy of Sciences of the United States of America 120 (11): e2208839120.
                </li>
                <li>
                    Hans, Abhimanyu, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. <a href="http://arxiv.org/abs/2401.12070">Spotting LLMs with Binoculars: Zero-Shot Detection of Machine-Generated Text.</a> arXiv [Cs.CL].
                </li>
                <li>
                    Su, Jinyan, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. <a href="https://arxiv.org/abs/2306.05540">DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text.</a> arXiv [Cs.CL].
                </li>
                <li>
                    Mitchell, Eric, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. <a href="http://arxiv.org/abs/2301.11305">DetectGPT: Zero-Shot Machine-Generated Text Detection Using Probability Curvature.</a> arXiv [Cs.CL].
                </li>
                <li>
                    Bao, Guangsheng, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2023. <a href="https://arxiv.org/abs/2310.05130">Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature.</a> arXiv [Cs.CL].
                </li>
                <li>
                    Koppel, Moshe, and Jonathan Schler. 2004. <a href="https://dl.acm.org/doi/abs/10.1145/1015330.1015448">Authorship Verification as a One-Class Classification Problem.</a> In Proceedings, Twenty-First International Conference on Machine Learning, ICML 2004, 489–95.
                </li>
                <li>
                    Bevendorff, Janek, Benno Stein, Matthias Hagen, and Martin Potthast. 2019. <a href="https://webis.de/publications.html#bevendorff_2019a">Generalizing Unmasking for Short Texts.</a> In Proceedings of the 2019 Conference of the North, 654–59. Stroudsburg, PA, USA: Association for Computational Linguistics.
                </li>
                <li>
                    Sculley, D., and C. E. Brodley. 2006. <a href="https://ieeexplore.ieee.org/abstract/document/1607268">Compression and Machine Learning: A New Perspective on Feature Space Vectors.</a> In Data Compression Conference (DCC’06), 332–41. IEEE.
                </li>
                <li>
                    Halvani, Oren, Christian Winter, and Lukas Graner. 2017. <a href="https://dl.acm.org/doi/abs/10.1145/3098954.3104050">On the Usefulness of Compression Models for Authorship Verification.</a> In ACM International Conference Proceeding Series. Vol. Part F1305. Association for Computing Machinery. https://doi.org/10.1145/3098954.3104050.
                </li>
                <li>
                    Uchendu, Adaku, Zeyu Ma, Thai Le, Rui Zhang, and Dongwon Lee. 2021. <a href="https://aclanthology.org/2021.findings-emnlp.172/">TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation.</a> In Findings of the Association for Computational Linguistics: EMNLP 2021, 2001–16. Stroudsburg, PA, USA: Association for Computational Linguistics.
                </li>
                <li>
                    Schuster, Tal, Roei Schuster, Darsh J. Shah, and Regina Barzilay. 2020. <a href="https://direct.mit.edu/coli/article/46/2/499/93369/The-Limitations-of-Stylometry-for-Detecting">The Limitations of Stylometry for Detecting Machine-Generated Fake News.</a> Computational Linguistics 46 (2): 499–510.
                </li>
                <li>
                    Sadasivan, Vinu Sankar, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. <a href="http://arxiv.org/abs/2303.11156">Can AI-Generated Text Be Reliably Detected?</a> arXiv [Cs.CL].
                </li>
                <li>
                    Ippolito, Daphne, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. <a href="https://aclanthology.org/2020.acl-main.164/">Automatic Detection of Generated Text Is Easiest When Humans Are Fooled.</a> In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 1808–22. Stroudsburg, PA, USA: Association for Computational Linguistics.
                </li>
            </ul>

            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/bevendorff.html %}
                {% include people-cards/wiegmann.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
                {% include people-cards/stamatatos.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2024 ows=true %}
            </div>
        </div>
    </div>
</main>
