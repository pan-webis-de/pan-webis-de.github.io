---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2018 - Cross-domain Authorship Attribution
description: PAN at CLEF 2018 - Cross-domain Authorship Attribution
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Cross-Domain Authorship Attribution 2018</a></li>
</ul>
</nav>


<main>
    <div class="uk-section uk-section-default">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Cross-Domain Authorship Attribution 2018</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#introduction">Introduction</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Development Phase</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation Phase</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#output">Output</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#performance-measures">Performance Measures</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#submission">Submission</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given a fanfiction text, determine its author among a list of candidates.</li>
                <li>Input: [<a href="{{ '../../data.html#pan18-authorship-attribution' | relative_url }}">data</a>]</li>
                <li>Output: [example] [schema] [verifier]</li>
                <li>Evaluation: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef18/authorship-attribution" target="_blank">code</a>]</li>
                <li>Submission: [<a href="https://www.tira.io/task/celebrity-profiling/">submit</a>]</li>
                <li>Baseline: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef18/authorship-attribution" target="_blank">code</a>]</li>
            </ul>

            <h2 id="introduction">Introduction</h2>
            <p>Authorship attribution is an important problem in information retrieval and computational linguistics but
                also in applied areas such as law and journalism where knowing the author of a document (such as a ransom
                note) may enable e.g. law enforcement to save lives. The most common framework for testing candidate
                algorithms is the <strong>closed-set attribution</strong> task: given a sample of reference documents from a
                restricted and finite set of candidate authors, the task is to determine the most likely author of a
                previously unseen document of unknown authorship? This task may be quite challenging when documents of known
                and unknown authorship come from different domains (e.g., thematic area, genre).
            <p>In this edition of PAN, for the first time, we focus on <strong>cross-domain attribution</strong>applied to
                <strong>Fanfiction</strong>. Fanfiction refers to fictional forms of literature which are nowadays produced
                by admirers ('fans') of a certain author (e.g. J.K. Rowling), novel ('Pride and Prejudice'), TV series
                (Sherlock Holmes), etc. The fans heavily borrow from the original work's theme, atmosphere, style,
                characters, story world etc. to produce new fictional literature, i.e. the so-called
                <strong>fanfics</strong>. This is why fanfiction is also known as transformative literature and has
                generated a number of controversies in recent years related to the intellectual rights property of the
                original authors (cf. plagiarism). Fanfiction, however, is typically produced by fans without any explicit
                commercial goals. The publication of fanfics typically happens online, on informal community platforms that
                are dedicated to making such literature accessible to a wider audience (e.g. <a  href=https://www.fanfiction.net>fanfiction.net</a>).
                The original work of art or genre is typically
                refered to as a <strong>fandom</strong>.</p>
            <p>The cross-domain attribution task in this edition of PAN can be more accurately described as
                <strong>cross-fandom attribution in fanfiction</strong>. In more detail, all documents of unknown authorship are fanfics of the
                same fandom (target fandom) while the documents of known authorship by the candidate authors are fanfics of
                several fandoms (other than the target-fandom).</p>


            <h2 id="task">Task</h2>
            <p>Given a set of documents (known fanfics) by a small number (up to 20) of candidate authors, identify the
            authors of another set of documents (unknown fanfics). Each candidate author has contributed at least one of
            the unknown fanfics, which all belong to the same target fandom. The known fanfics belong to several fandoms
            (excluding the target fandom), although not necessarily the same for all candidate authors. An equal number
            of fanfics per candidate author is provided. In contrast, the unknown fanfics are not equally distributed
            over the authors. The text-length of fanfics varies from 500 to 1,000 tokens. All documents are in the same
            language that may be <strong>English, French, Italian, Polish, or Spanish</strong>.</p>

            <h2 id="data">Development Phase</h2>
            <p>To develop your software, we provide you with a corpus with highly similar characteristics to the
                        evaluation corpus. It comprises a set of cross-domain authorship attribution problems in each of the
                        following 5 languages: English, French, Italian, Polish, and Spanish. Note that we specifically avoid to
                        use the term 'training corpus' because <strong>the sets of candidate authors of the development and the
                        evaluation corpora are not overlapping</strong>. Therefore, your approach should not be designed to
                        particularly handle the candidate authors of the development corpus. </p>
                    <p>Each problem consists of a set of known fanfics by each candidate author and a set of unknown fanfics
                        located in separate folders. The file <code>problem-info.json</code> that can be found in the main
                        folder of each problem, shows the name of folder of unknown documents and the list of names of candidate
                        author folders. </p>

                    <pre class="prettyprint"><code class="lang-json">{
    "unknown-folder": "unknown",
    "candidate-authors": [
        { "author-name": "candidate00001" },
        { "author-name": "candidate00002" },
        ...
    ]
}</code></pre>
                    <p>The true author of each unknown document can be seen in the file <code>ground-truth.json</code>, also found in
                        the main folder of each problem.</p>
                    <p>In addition, to handle a collection of such problems, the file <code>collection-info.json</code>includes
                        all relevant information. In more detail, for each problem it lists its main folder, the language
                        (either <code>"en"</code>, <code>"fr"</code>, <code>"it"</code>, <code>"pl"</code>, or
                        <code>"sp"</code>)
                        and encoding (always <code>UTF-8</code>) of its documents. </p>
                    <pre class="prettyprint"><code class="lang-json">[
    { "problem-name": "problem00001",
      "language": "en",
      "encoding": "UTF-8" },
    { "problem-name": "problem00002",
       "language": "fr",
       "encoding": "UTF-8" },
	  ...
]</code></pre>


            <h2 id="evaluation">Evaluation Phase</h2>
            <p>Once you finished tuning your approach to achieve satisfying performance on the development corpus, your
            software will be tested on the evaluation corpus. During the competition, the evaluation corpus will not
            be released publicly. Instead, we ask you to <strong>submit your software</strong> for evaluation at our
            site as described below.</p>
            <p>After the competition, the evaluation corpus will become available including ground truth data.
            This way, you have all the necessities to evaluate your approach on your own, yet being comparable to
            those who took part in the competition. </p>


            <h2 id="output">Output</h2>
            <p>Your system should produce one output file for each authorship attribution problem in <a
                href="http://www.json.org/">JSON</a>. The name of the output files should be <code>answers-PROBLEMNAME.json</code>
            (e.g., <code>answers-problem00001.json</code>, <code>answers-problem00002.json</code>)
            including the list of unknown documents and their predicted author:</p>

                        <pre class="prettyprint"><code class="lang-json">[
    { "unknown-text":  "unknown00001.txt",
      "predicted-author":  "candidate00003" },
    { "unknown-text":  "unknown00002.txt",
      "predicted-author":  "candidate00005" },
	...
]</code></pre>

            <h2 id="performance-measures">Performance Measures</h2>
            <p>The submissions will be evaluated in each attribution problem separately based on their
                <strong>macro-averaged F1 score</strong>. Participants will be ranked according to their
                average macro-F1 across all attribution problems of the evaluation corpus. </p>
            <p>We provide you with a Python script that calculates macro-F1 (and optionally the confusion
                matrix) of a single attribution problem.</p>

            <h2 id="submission">Submission</h2>
            <p>We ask you to prepare your software so that it can be executed via command line calls. The
                command shall take as input (i) an absolute path to the directory of the evaluation corpus and (ii)
                an absolute path to an existing empty output directory:</p>
            <pre class="prettyprint"><code class="lang-cmd">mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY</code></pre>
            <p>Within <code>EVALUATION-DIRECTORY</code> a <code>collection-info.json</code> file and a number of
                folders, one for each attribution problem, will be found (similar to the development corpus
                as described above). For each attribution problem, the output file should be written in
                <code>OUTPUT-DIRECTORY</code>.
            </p>
            <p><strong>Note:</strong> Each attribution problem should be solved independently of other
                problems in the collection.</p>
            <p>You can choose freely among the available programming languages and among the operating
                systems Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual
                machine that will be made accessible to you after registration. You will be able to reach the
                virtual machine via ssh and via remote desktop.</p>
            <p>Once deployed in your virtual machine, we ask you to access TIRA at <a
                    href="http://www.tira.io">www.tira.io</a>,
                where you can self-evaluate your software on the test data.</p>
            <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to
                grant us usage rights only for the purpose of the PAN competition. We agree not to share your
                software with a third party or use it for other purposes than the PAN competition.</p>

            <h2 id="related-work">Related Work</h2>
            <ul>
                <li>
                    <a href="{{ '/clef12/pan12/authorship-attribution.html' | relative_url }}">Author
                        identification task at PAN@CLEF'12</a> (closed-set authorship attribution)
                </li>
                <li>
                    <a href="{{ '/clef11/pan11/authorship-attribution.html' | relative_url }}">Author
                        identification task at PAN@CLEF'11</a> (closed-set authorship attribution)
                </li>
                <li>
                    Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                    Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3, March 2008.
                </li>
                <li>
                    Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                        href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                    Authorship Attribution</a>. Journal of the American Society for Information Science and
                    Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                </li>
                <li>
                    Efstathios Stamatatos. <a
                        href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                    Survey of Modern Authorship Attribution Methods</a>.
                    Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                    pages 538-556, March 2009.
                </li>
            </ul>


            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/kestemont.html %}
                {% include people-cards/stamatatos.html %}
                {% include people-cards/daelemans.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2018 %}
            </div>

        </div>  <!-- section -->
    </div>  <!-- section -->
</main>
