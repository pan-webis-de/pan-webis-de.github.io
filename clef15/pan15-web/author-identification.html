---
layout: default
nav_active: tasks
title: PAN at CLEF 2015 - Author Identification
description: PAN at CLEF 2015 - Author Identification
---

<main>
    <header class="uk-section uk-section-muted">

        <nav class="uk-container">
            <div class="uk-align-right uk-visible@m uk-text-muted">
                <a class="uk-button" href="{{ 'clef14/pan14-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-left"></a>
                CLEF 2015
                <a class="uk-button uk-padding-remove-right"
                   href="{{ 'clef16/pan16-web/author-identification.html' | relative_url }}"
                   data-uk-icon="chevron-right"></a>
            </div>

            <ul class="uk-text-muted uk-tab uk-margin-remove-top">
                <li><a href="{{ '/clef15/pan15-web/index.html' | relative_url }}">Overview</a></li>
                <li><a href="{{ '/clef15/pan15-web/keynotes.html' | relative_url }}">Keynotes</a></li>
                <li><a href="{{ '/clef15/pan15-web/program.html' | relative_url }}">Program</a></li>
                <li class="uk-active">
                    <a href="#">Tasks <span class="uk-margin-small-left"
                                            data-uk-icon="icon: chevron-down"></span></a>
                    <div class="uk-dropdown" data-uk-dropdown="mode: click">
                        <ul class="uk-nav uk-dropdown-nav">
                            <li class="uk-active"><a
                                    href="{{ '/clef15/pan15-web/author-identification.html' | relative_url }}">Author
                                Identification</a></li>
                            <li><a
                                    href="{{ '/clef15/pan15-web/author-profiling.html' | relative_url }}">Author
                                Profiling</a></li>
                            <li><a href="{{ '/clef15/pan15-web/plagiarism-detection.html' | relative_url }}">Plagiarism
                                Detection</a></li>
                        </ul>
                    </div>
                </li>
                <li><a href="{{ '/clef15/pan15-web/submission.html' | relative_url }}">Submission</a></li>
                <li><a href="{{ '/clef15/pan15-web/proceedings.html' | relative_url }}">Proceedings</a></li>
            </ul>
        </nav>

        <div class="uk-container uk-margin-small">
            {% include current-register-quicklink.html year=2015 %}
            <h1 class="uk-margin-small">Author Identification</h1>
            <div class="page-header-meta">PAN at CLEF 2015</div>
        </div>

        <div class="uk-container uk-margin-medium">
            <p class="uk-text-lead">This task focuses on <strong>authorship verification</strong> and methods to answer
                the question
                whether two given documents have the same author or no. This question accurately emulates the real-world
                problem that most forensic linguists face every day.</p>
        </div>
    </header>

    <section>
        <div class="uk-section">
            <div class="uk-container">
                <div class="uk-column-1-2@l uk-text-justify">
                    <h2>Author Identification</h2>
                    <p>
                        Authorship attribution is an important problem in many areas including
                        information retrieval and computational linguistics, but also in applied
                        areas such as law and journalism where knowing the author of a document
                        (such as a ransom note) may be able to save lives. The most common framework
                        for testing candidate algorithms is a text classification problem: given
                        known sample documents from a small, finite set of candidate authors, which
                        if any wrote a questioned document of unknown authorship? It has been
                        commented, however, that this may be an unreasonably easy task. A more
                        demanding problem is author verification where given a set of documents by a
                        single author and a questioned document, the problem is to determine if the
                        questioned document was written by that particular author or not. This may
                        more accurately reflect real life in the experiences of professional
                        forensic linguists, who are often called upon to answer this kind of
                        question. It is the third year PAN focuses on the so-called author
                        verification problem. The major difference with previous
                        PAN editions is that this year we no longer consider cases where all texts
                        within a verification problem are in the same genre or the same thematic
                        area. We are focusing on <strong>cross-genre and cross-topic author
                        verification</strong>, a more challenging version of the problem that better
                        resembles real-world applications.</p>
                    <p><strong>A note to forensic linguists:</strong> In order to bridge the gap between linguistics and
                        computer science, we strongly encourage submissions from researchers from both fields. We
                        understand that research groups with expertise in linguistics use manual or semi-automated
                        methods and, therefore, they are not able to submit their software. To enable their
                        participation, we will provide them with the opportunity to analyze the test corpus after the
                        deadline of software submission (mid-April). Their results will be ranked in a separate list
                        with respect to the performance of the software submissions and they will be entitled to
                        describe their approach in a paper.
                        In this framework, any scholar or research group with expertise in
                        linguistics wishing to participate should contact the Task Chair.
                    </p>


                    <h3>Task</h3>
                    <p>
                        Given a small set (no more than 5, possibly as few as one) of "known" documents by a single
                        person and a "questioned" document, the task is to determine whether the questioned document was
                        written by the same person who wrote the known document set.
                        The genre and/or topic may differ significantly between the known and
                        unknown documents.
                    </p>

                    <h3>Training Phase</h3>
                    <p>
                        To develop your software, we provide you with a training corpus that comprises a set of author
                        verification problems in several languages/genres. Each problem consists of some (up to five)
                        known documents by a single person and exactly one questioned document. All documents within a
                        single problem instance will be in the same language.
                        However, their genre and/or topic may differ
                        significantly. The document lengths vary from a few hundred to a few thousand words.
                    </p>
                    <a class="uk-button uk-button-primary" target="_blank"
                       href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-15/pan15-data/pan15-authorship-verification-training-dataset-2015-04-19.zip">Download
                        corpus</a> (Update April 19, 2015)
                    <p>
                        The documents of each problem are located in a separate folder,
                        the name of which (problem ID) encodes the language of the
                        documents. The following list shows the available sub-corpora, including their language, type
                        (cross-genre or cross-topic), code, and examples of problem IDs:
                    </p>
                    <table class="uk-table uk-table-divider uk-table-small uk-table-hover">
                        <thead>
                        <tr>
                            <th>Language</th>
                            <th>Type</th>
                            <th>Code</th>
                            <th>Problem IDs</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td>Dutch</td>
                            <td>
                                Cross-genre
                            </td>
                            <td>DU</td>
                            <td>
                                DU001, DU002,
                                DU003, etc.
                            </td>
                        </tr>
                        <tr>
                            <td>English</td>
                            <td>Cross-topic</td>
                            <td>EN</td>
                            <td>
                                EN001, EN002,
                                EN003, etc.
                            </td>
                        </tr>
                        <tr>
                            <td>Greek</td>
                            <td>Cross-topic</td>
                            <td>GR</td>
                            <td>GR001, GR002, GR003, etc.</td>
                        </tr>
                        <tr>
                            <td>Spanish</td>
                            <td>Cross-genre</td>
                            <td>SP</td>
                            <td>SP001, SP002, SP003, etc.</td>
                        </tr>
                        </tbody>
                    </table>

                    <p>
                        The ground truth data of the training corpus found in the file <code>truth.txt</code> include
                        one line per problem with problem ID and the correct binary answer (Y means the known and the
                        questioned documents are
                        by the same author and N means the opposite).
                        For example:
                    </p>
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
EN001 N
EN002 Y
EN003 N
...
		</pre>

                    <h3>Evaluation Phase</h3>
                    <p>
                        Once you finished tuning your approach to achieve satisfying performance on the training corpus,
                        your software will be tested on the evaluation corpus. During the competition, the evaluation
                        corpus
                        will not be released publicly. Instead, we ask you to submit your software for evaluation at our
                        site as described below.
                    </p>

                    <p>
                        After the competition, the evaluation corpus will become available including ground truth data.
                        This way, you have all the necessities to evaluate your approach on your own, yet being
                        comparable
                        to those who took part in the competition.
                    </p>

                    <h3>Output</h3>
                    Your software must take as input the absolute path to a set of problems. For each problem there is a
                    separate sub-folder within that path including the set of known documents and the single unknown
                    document of that problem (similarly to the training corpus). The software has to output a single
                    text file <code>answers.txt</code> with all the produced answers for the whole set of evaluation
                    problems. Each line of this file corresponds to a problem instance, it starts with the ID of the
                    problem followed by a score, a real number in [0,1] inclusive, corresponding to the probability of a
                    positive answer. That is, 0 means it is absolutely sure the questioned document is not by the author
                    of the known documents, 1.0 means it is absolutely sure the questioned document and the known
                    documents are by the same author, and 0.5 means that a positive and a negative answer are equally
                    likely. The probability scores should be round with three decimal digits. Use a single whitespace to
                    separate problem ID and probability score.
                    <br>For example, an <code>answers.txt</code> file may look like this:
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
EN001 0.031
EN002 0.874
EN003 0.500
...
</pre>

                    <h3>Performance Measures</h3>
                    <p>The participants' answers will be evaluated according to the area
                        under the ROC curve (AUC) of their probability scores.</p>
                    <p>In addition, the performance of the binary
                        classification results (automatically extracted from probability
                        scores where every score greater than 0.5 corresponds to a positive
                        answer, every score lower than 0.5 corresponds to a negative answer,
                        while 0.5 corresponds to an unanswered problem, or an "I don't know"
                        answer) will be measured based on c@1 (<a href="http://www.aclweb.org/anthology/P11-1142.pdf">Pe&ntilde;as
                            &amp; Rodrigo, 2011</a>):</p>
                    <ul>
                        <li>c@1 = (1/<i>n</i>)*(<i>n</i><sub>c</sub>+(<i>n</i><sub>u</sub>*<i>n</i><sub>c</sub>/<i>n</i>))
                        </li>
                    </ul>
                    <p>where:</p>
                    <ul>
                        <li><i>n</i> = #problems</li>
                        <li><i>n</i><sub>c </sub>= #correct_answers</li>
                        <li><i>n</i><sub>u </sub>= #unanswered_problems</li>
                    </ul>
                    <p><b>Note:</b> when
                        positive/negative answers are provided for all available problems
                        (probability scores different than 0.5), then c@1=accuracy. However,
                        c@1 rewards approaches that maintain the same number of correct
                        answers and decrease the number of incorrect answers by leaving some
                        problems unanswered (when probability score equals 0.5).</p>
                    The final ranking of the participants will be based on the <b>
                    product of AUC and c@1.</b>

                    <h3>Submission</h3>
                    <p>We ask you to prepare your software so that it can be executed via command line calls. To
                        maximize the sustainability of software submissions for this task, we encourage you to prepare
                        your software so it can be re-trained on demand, i.e., by offering two commands, one for
                        training, and one for testing. This way, your software can be reused on future evaluation
                        corpora as well as on private collections submitted to PAN by via our data submission
                        initiative.</p>
                    <p>The training command shall take as input (i) an absolute path to a training corpus formated as
                        described above, and (ii) an absolute path to an empty output directory:</p>
                    <pre class="prettyprint lang-c" style="overflow-x:auto">
> myTrainingSoftware <b>-i</b> path/to/training/corpus <b>-o</b> path/to/output/directory
</pre>
                    <p>Based on the training corpus, and perhaps based on its language and genre found within, your
                        software shall train a classification model, and save the trained model to the specified output
                        directory in serialized or binary form.</p>
                    <p>The testing command shall take as input (i) an absolute path to a test corpus (not containing the
                        ground truth) (ii) an absolute path to a previously trained classification model, and (iii) an
                        absolute path to an empty output directory:</p>
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
> myTestingSoftware <b>-i</b> path/to/test/corpus <b>-m</b> path/to/classification/model <b>-o</b> path/to/output/directory
</pre>
                    <p>Based on the classification model, the software shall classifiy each case found in the test
                        corpus and write an output file as described above to the output directory.</p>
                    <p>However, <b>offering a command for training is optional</b>, so if you face difficulties in doing
                        so, you may skip the training command and omit the model option <b>-m</b> from the testing
                        command.</p>
                    <p>You can choose freely among the available programming languages and among the operating systems
                        Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual machine
                        that will be made accessible to you after registration. You will be able to reach the virtual
                        machine via ssh and via remote desktop. More information about how to access the virtual
                        machines can be found in the user guide below:</p>
                    <p><a class="uk-button uk-button-primary" href="pan15-virtual-machine-user-guide.pdf">PAN Virtual
                        Machine User
                        Guide Â»</a></p>
                    <p>Once deployed in your virtual machine, we ask you to access TIRA at <a href="http://www.tira.io">www.tira.io</a>,
                        where you can self-evaluate your software on the test data.</p>
                    <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to grant
                        us usage rights only for the purpose of the PAN competition. We agree not to share your software
                        with a third party or use it for other purposes than the PAN competition.</p>

                    <h3>Test Corpus</h3>
                    <p>Once you finished tuning your approach to achieve satisfying performance on the training corpus,
                        you should run your software on the test corpus.</p>
                    <p>During the competition, the test corpus will not be released publicly. Instead, we ask you to
                        submit your software for evaluation at our site as described below.</p>
                    <p>After the competition, the test corpus is available including ground truth data. This way, you
                        have all the necessities to evaluate your approach on your own, yet being comparable to those
                        who took part in the competition.</p>
                    <p>
                        <a class="uk-button uk-button-primary" target="_blank"
                           href="https://www.uni-weimar.de/medien/webis/corpora/corpus-pan-labs-09-today/pan-15/pan15-data/pan15-authorship-verification-test-dataset2-2015-04-19.zip">Download
                            corpus</a>
                    </p>

                    <h3>Similarities/Differences with PAN-2014</h3>
                    For your convenience, we summarize here the main similarities/differences of the author
                    identification task @PAN-2015 with respect to the corresponding task @PAN-2014:
                    <br><br>Similarities:
                    <ul>
                        <li>The task definition is essentially the same</li>
                        <li>The format of corpus and ground truth is the same</li>
                        <li>The format of input/output of your software is the
                            same
                        </li>
                        <li>The positive/negative problems are equally distributed</li>
                        <li>The evaluation measures are the same</li>
                        <li>It is possible (optionally) to submit a trainable version of your approach to be used with
                            any given training corpus
                        </li>
                    </ul>
                    Differences:
                    <ul>
                        <li>The genre and/or topic of the documents within a
                            verification problem may differ significantly.
                        </li>
                    </ul>

                    <h3>Related Work</h3>
                    <p>We refer you to:</p>
                    <ul>
                        <li>
                            <a href="../../clef14/pan14-web/about.html#proceedings">PAN @ CLEF'14</a> (<a
                                href="https://www.uni-weimar.de/medien/webis/events/pan-14/pan14-papers-final/pan14-authorship-verification/stamatatos14-overview.pdf">overview
                            paper</a>)
                        </li>
                        <li>
                            <a href="../../clef13/pan13-web/about.html#proceedings">PAN @ CLEF'13</a> (<a
                                href="https://www.uni-weimar.de/medien/webis/events/pan-13/pan13-papers-final/pan13-authorship-verification/juola13-overview.pdf">overview
                            paper</a>)
                        </li>
                        <li>
                            <a href="../../clef12/pan12-web/about.html#proceedings">PAN @ CLEF'12</a> (<a
                                href="https://www.uni-weimar.de/medien/webis/events/pan-12/pan12-papers-final/pan12-author-identification/juola12-overview.pdf">overview
                            paper</a>)
                        </li>
                        <li>
                            <a href="../../clef11/pan11-web/about.html#proceedings">PAN @ CLEF'11</a> (<a
                                href="https://www.uni-weimar.de/medien/webis/events/pan-11/pan11-papers-final/pan11-author-identification/argamon11-overview.pdf">overview
                            paper</a>)
                        </li>
                        <li>
                            Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                            Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                            March 2008.
                        </li>
                        <li>
                            Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                                href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                            Authorship Attribution</a>. Journal of the American Society for Information Science and
                            Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                        </li>
                        <li>
                            Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                            Survey of Modern Authorship Attribution Methods</a>.
                            Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                            pages 538-556, March 2009.
                        </li>
                    </ul>

                </div>
            </div>
        </div>

        <div class="uk-section uk-section-muted">
            <div class="uk-container">

                <h2>Task Chair</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/stamatatos.html %}
                </div>

                <h2>Task Committee</h2>
                <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                    {% include people-cards/daelemans.html %}
                    {% include people-cards/juola.html %}
                    {% include people-cards/potthast.html %}
                    {% include people-cards/stein.html %}
                    {% include people-cards/verhoeven.html %}
                    {% include people-cards/lopez.html %}
                </div>
            </div>
        </div>
    </section>
</main>
