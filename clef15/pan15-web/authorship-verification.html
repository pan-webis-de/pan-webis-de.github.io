---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2015 - Authorship Verification
description: PAN at CLEF 2015 - Author Verification
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Authorship Verification 2015</a></li>
</ul>
</nav>

<main>
    <div class="uk-section uk-section-default">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Authorship Verification 2015</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#introduction">Introduction</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Data</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#output">Output</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given two documents, are they written by the same author?</li>
                <li>Input: [<a href="{{ 'data.html#pan15-verification' | relative_url }}">data</a>]</li>
                <li>Evaluation: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef15/author-clustering" target="_blank">code</a>]</li>
                <li>Submission: [<a href="https://www.tira.io/task/author-identification/">submit</a>]</li>
            </ul>

            <h2 id="introduction">Introduction</h2>
            <p>
                Authorship attribution is an important problem in many areas including
                information retrieval and computational linguistics, but also in applied
                areas such as law and journalism where knowing the author of a document
                (such as a ransom note) may be able to save lives. The most common framework
                for testing candidate algorithms is a text classification problem: given
                known sample documents from a small, finite set of candidate authors, which
                if any wrote a questioned document of unknown authorship? It has been
                commented, however, that this may be an unreasonably easy task. A more
                demanding problem is author verification where given a set of documents by a
                single author and a questioned document, the problem is to determine if the
                questioned document was written by that particular author or not. This may
                more accurately reflect real life in the experiences of professional
                forensic linguists, who are often called upon to answer this kind of
                question. It is the third year PAN focuses on the so-called author
                verification problem. The major difference with previous
                PAN editions is that this year we no longer consider cases where all texts
                within a verification problem are in the same genre or the same thematic
                area. We are focusing on <strong>cross-genre and cross-topic author
                verification</strong>, a more challenging version of the problem that better
                resembles real-world applications.</p>
            <p><strong>A note to forensic linguists:</strong> In order to bridge the gap between linguistics and
                computer science, we strongly encourage submissions from researchers from both fields. We
                understand that research groups with expertise in linguistics use manual or semi-automated
                methods and, therefore, they are not able to submit their software. To enable their
                participation, we will provide them with the opportunity to analyze the test corpus after the
                deadline of software submission (mid-April). Their results will be ranked in a separate list
                with respect to the performance of the software submissions and they will be entitled to
                describe their approach in a paper.
                In this framework, any scholar or research group with expertise in
                linguistics wishing to participate should contact the Task Chair.
            </p>


            <h2 id="task">Task</h2>
            <p>
                Given a small set (no more than 5, possibly as few as one) of "known" documents by a single
                person and a "questioned" document, the task is to determine whether the questioned document was
                written by the same person who wrote the known document set.
                The genre and/or topic may differ significantly between the known and
                unknown documents.
            </p>

            <h2 id="data">Data</h2>
            <p>
                To develop your software, we provide you with a training corpus that comprises a set of author
                verification problems in several languages/genres. Each problem consists of some (up to five)
                known documents by a single person and exactly one questioned document. All documents within a
                single problem instance will be in the same language.
                However, their genre and/or topic may differ
                significantly. The document lengths vary from a few hundred to a few thousand words.
            </p>
            <p>
                The documents of each problem are located in a separate folder,
                the name of which (problem ID) encodes the language of the
                documents. The following list shows the available sub-corpora, including their language, type
                (cross-genre or cross-topic), code, and examples of problem IDs:
            </p>
            <table class="uk-table uk-table-divider uk-table-small uk-table-hover">
                <thead>
                <tr>
                    <th>Language</th>
                    <th>Type</th>
                    <th>Code</th>
                    <th>Problem IDs</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Dutch</td>
                    <td>
                        Cross-genre
                    </td>
                    <td>DU</td>
                    <td>
                        DU001, DU002,
                        DU003, etc.
                    </td>
                </tr>
                <tr>
                    <td>English</td>
                    <td>Cross-topic</td>
                    <td>EN</td>
                    <td>
                        EN001, EN002,
                        EN003, etc.
                    </td>
                </tr>
                <tr>
                    <td>Greek</td>
                    <td>Cross-topic</td>
                    <td>GR</td>
                    <td>GR001, GR002, GR003, etc.</td>
                </tr>
                <tr>
                    <td>Spanish</td>
                    <td>Cross-genre</td>
                    <td>SP</td>
                    <td>SP001, SP002, SP003, etc.</td>
                </tr>
                </tbody>
            </table>
            <p>
                The ground truth data of the training corpus found in the file <code>truth.txt</code> include
                one line per problem with problem ID and the correct binary answer (Y means the known and the
                questioned documents are
                by the same author and N means the opposite).
                For example:
            </p>
            <pre class="prettyprint lang-py" style="overflow-x:auto">
EN001 N
EN002 Y
EN003 N
...</pre>

            <h2 id="output">Output</h2>
            Your software must take as input the absolute path to a set of problems. For each problem there is a
            separate sub-folder within that path including the set of known documents and the single unknown
            document of that problem (similarly to the training corpus). The software has to output a single
            text file <code>answers.txt</code> with all the produced answers for the whole set of evaluation
            problems. Each line of this file corresponds to a problem instance, it starts with the ID of the
            problem followed by a score, a real number in [0,1] inclusive, corresponding to the probability of a
            positive answer. That is, 0 means it is absolutely sure the questioned document is not by the author
            of the known documents, 1.0 means it is absolutely sure the questioned document and the known
            documents are by the same author, and 0.5 means that a positive and a negative answer are equally
            likely. The probability scores should be round with three decimal digits. Use a single whitespace to
            separate problem ID and probability score.
            <br>
            For example, an <code>answers.txt</code> file may look like this:
            <pre class="prettyprint lang-py" style="overflow-x:auto">
EN001 0.031
EN002 0.874
EN003 0.500
...
</pre>


            <h2 id="evaluation">Evaluation</h2>
            <p>Once you finished tuning your approach to achieve satisfying performance on the training corpus,
            your software will be tested on the evaluation corpus. During the competition, the evaluation corpus
            will not be released publicly. Instead, we ask you to submit your software for evaluation at our
            site as described below.</p>

            <p>After the competition, the evaluation corpus will become available including ground truth data.
            This way, you have all the necessities to evaluate your approach on your own, yet being
            comparable to those who took part in the competition.</p>
            <ul>
                <li>The clustering output will be evaluated according to <strong>BCubed F-score</strong> (<a href="http://nlp.uned.es/docs/amigo2007a.pdf">Amigo et al. 2007</a>)
                </li>
                <li>The ranking of authorship links will be evaluated according to <strong>Mean Average Precision</strong>
                    (<a href="http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html">Manning et al. 2008</a>)
                </li>
            </ul>
            <p>For your convenience, we provide an evaluator script written in Octave.</p>
            <p>It takes three parameters: (-i) an input directory (the data set including a 'truth' folder),
                (-a) an answers directory (your software output) and (-o) an output directory where the
                evaluation results are written to. Of course, you are free to modify the script according to
                your needs.</p>

            <p>The participants' answers will be evaluated according to the area under the ROC curve (AUC) of their probability scores.</p>
            <p>In addition, the performance of the binary classification results (automatically extracted from probability
                scores where every score greater than 0.5 corresponds to a positive answer, every score lower than 0.5 corresponds to a negative answer,
                while 0.5 corresponds to an unanswered problem, or an "I don't know" answer) will be measured based on c@1
                (<a href="http://www.aclweb.org/anthology/P11-1142.pdf">Pe&ntilde;as &amp; Rodrigo, 2011</a>):</p>
            <ul>
                <li>c@1 = (1/<i>n</i>)*(<i>n</i><sub>c</sub>+(<i>n</i><sub>u</sub>*<i>n</i><sub>c</sub>/<i>n</i>))</li>
            </ul>
            <p>where:</p>
            <ul>
                <li><i>n</i> = #problems</li>
                <li><i>n</i><sub>c </sub>= #correct_answers</li>
                <li><i>n</i><sub>u </sub>= #unanswered_problems</li>
            </ul>
            <p><b>Note:</b> when positive/negative answers are provided for all available problems
            (probability scores different than 0.5), then c@1=accuracy. However, c@1 rewards approaches that maintain the same number of correct
            answers and decrease the number of incorrect answers by leaving some problems unanswered (when probability score equals 0.5).</p>
            The final ranking of the participants will be based on the <b> product of AUC and c@1.</b>

            <h2 id="related-work">Related Work</h2>
            <ul>
                <li>
                    <a href="{{ 'publications.html#?q=2014%20stamatatos' | relative_url }}">Author Identification, PAN @ CLEF'14</a>
                </li>
                <li>
                    <a href="{{ 'publications.html#?q=2013%20juola' | relative_url }}">Author Identification, PAN @ CLEF'13</a>
                </li>
                <li>
                    <a href="{{ 'publications.html#?q=2012%20juola' | relative_url }}">Author Identification, PAN @ CLEF'12</a>
                </li>
                <li>
                    <a href="{{ 'publications.html#?q=2011%20argamon' | relative_url }}">Author Identification, PAN @ CLEF'11</a>
                </li>
                <li>
                    Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                    Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                    March 2008.
                </li>
                <li>
                    Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                        href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                    Authorship Attribution</a>. Journal of the American Society for Information Science and
                    Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                </li>
                <li>
                    Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                    Survey of Modern Authorship Attribution Methods</a>.
                    Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                    pages 538-556, March 2009.
                </li>
            </ul>


            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/stamatatos.html %}
                {% include people-cards/daelemans.html %}
                {% include people-cards/juola.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
                {% include people-cards/verhoeven.html %}
                {% include people-cards/lopez.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2015 %}
            </div>

        </div>  <!-- section -->
    </div>  <!-- section -->
</main>
