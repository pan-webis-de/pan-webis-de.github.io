---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2017 - Authorship Clustering
description: PAN at CLEF 2017 - Authorship Clustering
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Authorship Clustering 2017</a></li>
</ul>
</nav>

<main>
    <div class="uk-section uk-section-default">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Authorship Clustering 2017</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#introduction">Introduction</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Development Phase</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation Phase</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#output">Output</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#performance-measures">Performance Measures</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given a document collection, cluster the texts by authorship.</li>
                <li>Input: [<a href="{{ '../../data.html#pan17-clustering' | relative_url }}">data</a>]</li>
                <li>Output: [example] [schema] [verifier]</li>
                <li>Evaluation: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef17/author-clustering" target="_blank">code</a>]</li>
                <li>Submission: [<a href="https://www.tira.io/task/author-clustering/">submit</a>]</li>
            </ul>

            <h2 id="introduction">Introduction</h2>
            <p>Authorship attribution is an important problem in information retrieval and computational
                linguistics but also in applied areas such as law and journalism where knowing the author of a
                document (such as a ransom note) may be able to save lives. The most common framework for
                testing candidate algorithms is the closed-set attribution task: given known sample documents
                from a small, finite set of candidate authors, which wrote a questioned document of unknown
                authorship? It has been commented, however, that this may be an unreasonably easy task. A more
                demanding task is author clustering where given a document collection the task is to group
                documents written by the same author so that each cluster corresponds to a different author.
                This task can also be viewed as establishing authorship links between documents. </p>
            <p>Note that the task of authorship verification studied in detail in previous editions of PAN
                (2013-2015) is strongly associated with author clustering since any clustering problem can be
                decomposed into a series of author verification problems. We encourage participants to attempt
                to modify authorship verification approaches to deal with the author clustering task.</p>
            <p>PAN-2016 first studied the author clustering task focusing on relatively long documents like
                articles and reviews. In this edition of PAN, we focus on short documents of paragraph length.
                The aim is to cluster paragraphs that may be extracted from the same document or different
                documents and are by the same author. Such a task is closely related to author diarization and
                intrinsic plagiarism detection. </p>
            <p>Similar to PAN-2016 edition of the task, two application scenarios are examined: </p>
            <ul>
                <li><strong>Complete author clustering</strong>: This scenario requires a detailed analysis
                    where the number (k) of different authors found in the collection should be identified and
                    each document should be assigned to exactly one of the k clusters (each cluster corresponds
                    to a different author). In the following illustrating example, 4 different authors are found
                    and the colour of each document indicates its author.
                </li>
                <p align="center"><img
                        src="https://cloud.githubusercontent.com/assets/15824066/12065874/81d95440-afe7-11e5-828b-54e293540823.png"/>
                </p>
                <li><strong>Authorship-link ranking</strong>: This scenario views the exploration of the given
                    document collection as a retrieval task. It aims at establishing authorship links between
                    documents and provides a list of document pairs ranked according to a confidence score (the
                    score shows how likely it is the document pair to be by the same author). In the following
                    example, 4 document pairs with similar authorship are found and then these authorship-links
                    are sorted according to their similarity.
                </li>
                <p align="center"><img
                        src="https://cloud.githubusercontent.com/assets/15824066/12065971/f2a087e2-afe8-11e5-9ef4-6df4e5aff9ae.png"/>
                </p>
            </ul>


            <h2 id="task">Task</h2>
            <p>Given a collection of (up to 50) short documents (paragraphs extracted from larger documents),
                identify authorship links and groups of documents by the same author. All documents are
                single-authored, in the same language, and belong to the same genre. However, the topic or
                text-length of documents may vary. The number of distinct authors whose documents are included
                in the collection is not given.
            </p>

            <h2 id="data">Development Phase</h2>
            <p>To develop your software, we provide you with a training corpus that comprises a set of author
                clustering problems in <strong>3 languages</strong> (English, Dutch, and Greek) and <strong>2
                    genres</strong> (newspaper articles and reviews). Each problem consists of a set of
                documents in the same language and genre. However, their topic may differ and the document
                lengths vary from a few hundred to a few thousand words.</p>
            <p>The documents of each problem are located in a separate folder. The file <code>info.json</code>
                describes all required information for each clustering problem. In more detail, the language
                (either <code>"en"</code>, <code>"nl"</code>, or <code>"gr"</code> for English, Dutch and Greek,
                respectively), genre (either <code>"articles"</code> or <code>"reviews"</code>), and the folder
                of each problem (relative path).</p>
            <pre class="prettyprint lang-py" style="overflow-x:auto">
[
   {"language": "en", "genre": "articles", "folder": "problem001"},
   ...
]
	</pre>
            <p>The ground truth data of the training corpus consists of two files for each clustering problem:
                <code>clustering.json</code> and <code>ranking.json</code> similar to the files described in the
                <strong>Output</strong> section (see details below). All ground truth files are located in the
                <code>truth</code> folder.</p>


            <h2 id="evaluation">Evaluation Phase</h2>
             <p>Once you finished tuning your approach to achieve satisfying performance on the training corpus,
                your software will be tested on the evaluation corpus. During the competition, the evaluation
                corpus
                will not be released publicly. Instead, we ask you to <strong>submit your software</strong> for
                evaluation at our site as described below.</p>
            <p>After the competition, the evaluation corpus will become available including ground truth data.
                This way, you have all the necessities to evaluate your approach on your own, yet being
                comparable
                to those who took part in the competition.</p>


            <h2 id="output">Output</h2>
             <p>Your system should produce <strong>two output files</strong> in <a href="http://www.json.org/">JSON</a>:
                    </p>
                    <ul>
                        <li>One output file including complete information about the detected clusters named <code>clustering.json</code>.
                            Each cluster should contain all documents found in the collection by a specific author. A
                            JSON file of the following format should be produced (a list of clusters, each cluster is a
                            list of documents):
                        <li>
<pre class="prettyprint lang-py" style="overflow-x:auto">
[
	[
		{"document":  "filename1"},
		{"document":  "filename2"},
	…
	],
…
]
</pre>

                            <p>The clusters should be non-overlapping, thus each filename should belong to exactly one
                                cluster.</p>

                        <li>One output file named <code>ranking.json</code> including a list of document pairs ranked
                            according to a real-valued score in [0,1], where higher values denote more confidence that
                            the pair of documents are by the same author. A JSON file of the following format should be
                            produced (a list of document pairs and a real-valued number):
                        </li>
                        <pre class="prettyprint lang-py" style="overflow-x:auto">
[
	{"document1": "filename1",
	 "document2": "filename2",
	 "score": real-valued-number},
	…
]
</pre>
                        <p>The order of documents within a pair is not important (e.g. "document1": "filename1",
                            "document2": "filename2" is the same with "document2": "filename1",
                            "document1": "filename2"). In case the same pair is reported more than once the first
                            occurrence will be taken into account.</p>
                    </ul>

                    <p>An <strong>illustrating example</strong> follows. Let’s assume that a document collection of 6
                        files is given: file1.txt, file2.txt, file3.txt, file4.txt, file5.txt, and file6.txt. There are
                        3 clusters: (i) file1.txt, file3.txt, and file4.txt are by the same author, (ii) file5.txt and
                        file6.txt are by another author and (iii) file2.txt is by yet another author. </p>
                    <ul>
                        <li>The output file in JSON for the complete author clustering task should be:</li>
                        <pre class="prettyprint lang-py" style="overflow-x:auto">
[   [	{"document": "file1.txt"},
		{"document": "file3.txt"},
		{"document": "file4.txt"}	],
	[
		{"document": "file5.txt"},
		{"document": "file6.txt"}	],
	[
		{"document": "file2.txt"}	]
]
</pre>
                        <li>An example of the output file for authorship-link ranking could be:</li>
                        <pre class="prettyprint lang-py" style="overflow-x:auto">
[	{"document1": "file1.txt",
	 "document2": "file4.txt",
	 "score": 0.95},

	{"document1": "file3.txt",
	 "document2": "file4.txt",
	 "score": 0.75},

	{"document1": "file5.txt",
	 "document2": "file6.txt",
	 "score": 0.66},

	{"document1": "file1.txt",
	 "document2": "file3.txt",
	 "score": 0.63}
]
</pre>
                    </ul>


            <h2 id="performance-measures">Performance Measures</h2>
            <ul>
                <li>The clustering output will be evaluated according to <strong>BCubed F-score</strong> (<a
                        href="http://nlp.uned.es/docs/amigo2007a.pdf">Amigo et al. 2007</a>)
                </li>
                <li>The ranking of authorship links will be evaluated according to <strong>Mean Average
                    Precision</strong> (<a
                        href="http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html">Manning
                    et al. 2008</a>)
                </li>
            </ul>
            <p>For your convenience, we provide an evaluator script written in Octave.</p>
            <p>It takes three parameters: (-i) an input directory (the data set including a 'truth' folder),
                (-a) an answers directory (your software output) and (-o) an output directory where the
                evaluation results are written to. Of course, you are free to modify the script according to
                your needs.</p>


            <h2 id="related-work">Related Work</h2>
            <ul>
                <li>
                    <a href="{{ 'publications.html#?q=2016%20stamatatos' | relative_url }}">Author Clustering, PAN@CLEF'16</a>
                </li>
                <li>
                    <a href="{{ 'publications.html#?q=2015%20stamatatos' | relative_url }}">Author Verification, PAN @ CLEF'15</a>
                </li>
                <li>
                    <a href="{{ 'publications.html#?q=2014%20stamatatos' | relative_url }}">Author Verification, PAN @ CLEF'14</a>
                </li>
                <li>
                    <a href="{{ 'publications.html#?q=2013%20juola' | relative_url }}">Author Verification, PAN @ CLEF'13</a>
                </li>
                <li>
                    Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                    Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                    March 2008.
                </li>
                <li>
                    Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                        href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                    Authorship Attribution</a>. Journal of the American Society for Information Science and
                    Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                </li>
                <li>
                    Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                    Survey of Modern Authorship Attribution Methods</a>.
                    Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                    pages 538-556, March 2009.
                </li>
            </ul>


            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/stamatatos.html %}
                {% include people-cards/daelemans.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
                {% include people-cards/verhoeven.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2017 %}
            </div>

        </div>  <!-- section -->
    </div>  <!-- section -->
</main>
