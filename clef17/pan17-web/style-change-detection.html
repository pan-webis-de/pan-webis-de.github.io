---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2017 - Style Change Detection
description: PAN at CLEF 2017 - Style Change Detection
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Style Change Detection 2017</a></li>
</ul>
</nav>

<main class="uk-section uk-section-default">
    <div class="uk-container"">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Style Change Detection 2017</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#introduction">Introduction</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Development Phase</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#output">Output</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#performance-measures">Performance Measures</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>
        <div class="uk-container uk-margin-medium">
            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given a document, determine whether it contains style changes or not.</li>
                <li>Input: [<a href="{{ '../../data.html#pan17-style-change-detection' | relative_url }}">data</a>]</li>
                <li>Evaluation: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef17/style-breach-detection" target="_blank">code</a>]</li>
                <li>Submission: [<a href="https://www.tira.io/task/style-breach-detection/">submit</a>]</li>
            </ul>

            <h2 id="introduction">Introduction</h2>
            <p>While many approaches target the problem of identifying authors of whole documents, research on
                investigating multi-authored documents is sparse. To narrow the gap, the author diarization task of
                the PAN-2016 edition already focused on collaboratively written documents, attempting to cluster
                text by authors within documents. This year we modify the problem by asking participants to
                detect style breaches within documents, i.e., to locate borders where authorships change.</p>
            <p>The problem is therefore related to the <strong>text segmentation</strong> problem, with the
                difference that the latter usually focus on detecting switches of topics or <i>stories</i>. In
                contrast to that, this task aims to find borders based on the writing style, disregarding the
                specific content. As the goal is to only find borders, it is irrelevant to identify or cluster
                authors of segments. A simple example consisting of four breaches of style (switches / borders)
                is illustrated below:</p>
            <center><img src="../pan17-figures/style-breach-sample.png"/></center>


            <h2 id="task">Task</h2>
            <p>Given a document, determine whether it is multi-authored, and if yes, find the borders where
                authors switch.</p>
            <p>All documents are provided in English and may contain zero up to arbitrarily many switches (style
                breaches). Thereby switches of authorships may only occur at the end of sentences, i.e., not
                within.</p>


            <h2 id="data">Development Phase</h2>
            <p>To develop your algorithms, a training data set including corresponding solutions is
                provided.</p>
                <br/>
            <p>For each problem instance X, two files are provided:</p>
            <ul>
                <li><code>problem-X.txt</code> contains the actual text</li>
                <li><code>problem-X.truth</code> contains the ground truth, i.e., the correct solution in <a
                        href="http://www.json.org/">JSON</a> format:
                    <pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": [
        character_position_border_1,
        character_position_border_2,
        …
    ]
}
</pre>

                <p>
                    To identify a border, the absolute character position <strong>of the first
                    non-whitespace character of the new segment</strong> is used. The document starts at
                    character position 0. An example that could match the borders of the image above could
                    look as follows:</p>
                <pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": [1709, 3119, 3956, 5671]
}
</pre>

                <p>An empty array indicates that the document is single-authored, i.e., contains no style
                    switches:</p>
                <pre class="prettyprint lang-py" style="overflow-x:auto">
{
    "borders": []
}
</pre>
                </li>
            </ul>


            <h2>Evaluation Phase</h2>
            Once you finished tuning your approach to achieve satisfying performance on the training corpus,
            your software will be tested on the evaluation corpus. During the competition, the evaluation corpus
            will not be released publicly. Instead, we ask you to <strong>submit your software</strong> for
            evaluation at our site as described below.
            <br>After the competition, the evaluation corpus will become available including ground truth data.
            This way, you have all the necessities to evaluate your approach on your own, yet being comparable
            to those who took part in the competition.
            <p></p>

            <h2 id="output">Output</h2>
            <p>
                In general, the data structure during the evaluation phase will be similar to that in the
                training phase, with the exception that the ground truth files are missing. Thus, for each given
                problem <code>problem-X.txt</code> your software should output the missing solution file <code>problem-X.truth</code>.
                The output syntax should thereby be exactly like it is described in the training phase section.
            </p>


            <h2 id="performance-measures">Performance Measures</h2>
            <p>To evaluate the predicted style breaches, two metrics will be used:
            <ul>
                <li>the <strong>WindowDiff</strong> metric (<a
                        href="http://people.ischool.berkeley.edu/~hearst/papers/pevzner-01.pdf">Pevzner, Hearst,
                    2002</a>) was proposed for general text segmentation evaluation and is still the de facto
                    standard for such problems. It gives an error rate (between 0 to 1, 0 indicating a perfect
                    prediction) for predicting borders by penalizing near-misses less than other/complete misses
                    or extra borders.
                </li>
                <li>a more recent adaption of the WindowDiff metric is the <strong>WinPR</strong> metric (<a
                        href="http://www.aclweb.org/anthology/N12-1038.pdf">Scaiano, Inkpen, 2012</a>). It
                    enhances it by computing the common information retrieval measures precision (WinP) and
                    recall (WinR) and thus allows to give a more detailed, qualitative statement about the
                    prediction. For the final ranking of all participating teams, the F-score of WinPR will be
                    used.
                </li>
            </ul>

            <p>Note that while both metrics will be computed on a word-level, you still have to provide your
                solutions on a character-level (delegating the tokenization to the evaluator).</p>
            <p>For your convenience, we provide the evaluator script written in Python.
                It takes three parameters: an input directory (the data set), an inputRun directory (your
                computed breaches) and an output directory where the results file is written to. Of course, you
                are free to modify the script according to your needs.</p>


            <h2 id="related-work">Related Work</h2>
            <ul>
                <li>
                    <a href="{{ 'publications.html#?q=2016%20stamatatos' | relative_url }}">Author Clustering, PAN@CLEF'16</a>
                </li>
                <li>Marti A. Hearst. <a href="http://anthology.aclweb.org/J/J97/J97-1003.pdf">TextTiling:
                    Segmenting Text into Multi-paragraph Subtopic Passages.</a>. In Computational Linguistics,
                    Volume 23, Issue 1, pages 33-64, 1997.
                </li>
                <li>Benno Stein, Nedim Lipka and Peter Prettenhofer. <a
                        href="https://www.uni-weimar.de/medien/webis/publications/papers/stein_2011a.pdf">Intrinsic
                    Plagiarism Analysis</a>. In Language Resources and Evaluation, Volume 45, Issue 1, pages
                    63–82, 2011.
                </li>
                <li>
                    Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                    Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                    March 2008.
                </li>
                <li>
                    Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                    Survey of Modern Authorship Attribution Methods</a>.
                    Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                    pages 538-556, March 2009.
                </li>
            </ul>


            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/tschuggnall.html %}
                {% include people-cards/specht.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2017 %}
            </div>

        </div>  <!-- container -->
    </div>  <!-- section -->
</main>

<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>
