---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2014 - Authorship Verification
description: PAN at CLEF 2014 - Authorship Verification
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Authorship Verification 2014</a></li>
</ul>
</nav>

<main class="uk-section uk-section-default">
    <div class="uk-container"">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Authorship Verification 2014</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#introduction">Introduction</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#input">Input</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#output">Output</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#results">Results</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given two documents, are they written by the same author?</li>
                <li>Input: [<a href="{{ 'data.html#pan14-verification' | relative_url }}">data</a>]</li>
                <li>Submission: [<a href="https://www.tira.io/task/author-identification/">submit</a>]</li>
            </ul>

            <h2 id="introduction">Introduction</h2>
            <p>Authorship attribution is an important problem in many areas including information retrieval and
            computational linguistics, but also in applied areas such as law and journalism where knowing
            the author of a document (such as a ransom note) may be able to save lives. The most common
            framework for testing candidate algorithms is a text classification problem: given known sample
            documents from a small, finite set of candidate authors, which if any wrote a questioned
            document of unknown authorship? It has been commented, however, that this may be an unreasonably
            easy task. A more demanding problem is author verification where given a set of documents by a
            single author and a questioned document, the problem is to determine if the questioned document
            was written by that particular author or not. This may more accurately reflect real life in the
            experiences of professional forensic linguists, who are often called upon to answer this kind of
            question. It is the second year PAN focuses on the so-called author verification problem.</p>
            <p><strong>A note to forensic linguists:</strong> In order to bridge the gap between linguistics and
            computer science, we strongly encourage submissions from researchers from both fields. We
            understand that research groups with expertise in linguistics use manual or semi-automated
            methods and, therefore, they are not able to submit their software. To enable their
            participation, we will provide them with the opportunity to analyze the test corpus after the
            deadline of software submission (mid-April). Their results will be ranked in a separate list
            with respect to the performance of the software submissions and they will be entitled to
            describe their approach in a paper. In this framework, any scholar or research group with expertise in
            linguistics wishing to participate should contact the Task Chair.</p>


            <h2 id="task">Task</h2>
            <p>
            Given a small set (no more than 5, possibly as few as one) of "known" documents by a single
            person and a "questioned" document, the task is to determine whether the questioned document was
            written by the same person who wrote the known document set.</p>
            <p>For your convenience, we summarize the main contributions of the 2014 edition of the author
            identification task with respect to previous editions:</p>
            Novelties:
            <ul>
                <li>The output of your software must be composed of real (probability) scores rather than binary
                    Y/N answers
                </li>
                <li>The maximum number of documents of known authorship within a problem is 5 (instead of 10)
                </li>
                <li>The evaluation measures used for ranking are (ROC) AUC and c@1 instead of recall, precision
                    and F1
                </li>
                <li>More languages/genres are represented in the corpus</li>
                <li>The training/evaluation corpora are larger</li>
                <li>It is possible (optionally) to submit a trainable version of your approach to be used with
                    any given training corpus
                </li>
            </ul>
            Unchanged:
            <ul>
                <li>The task definition is the same</li>
                <li>The format of corpus and ground truth is the same</li>
                <li>The positive/negative problems are equally distributed</li>
            </ul>


            <h2 id="input">Input</h2>
            <p>To develop your software, we provide you with a training corpus that comprises a set of author
            verification problems in several languages/genres. Each problem consists of some (up to five)
            known documents by a single person and exactly one questioned document. All documents within a
            single problem instance will be in the same language and best efforts are applied to assure that
            within-problem documents are matched for genre, register, theme, and date of writing. The
            document lengths vary from a few hundred to a few thousand words.</p>
            <p>The documents of each problem are located in a separate folder, the name of which (problem ID)
            encodes the language/genre of the documents. The following list shows the available
            languages/genres, their codes, and examples of problem IDs:</p>
            <table class="uk-table uk-table-divider uk-table-small uk-table-hover">
                <thead>
                <tr>
                    <th>Language</th>
                    <th>Genre</th>
                    <th>Code</th>
                    <th>Problem IDs</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Dutch</td>
                    <td>essays</td>
                    <td>DE</td>
                    <td>DE001, DE002, DE003, etc.</td>
                </tr>
                <tr>
                    <td>Dutch</td>
                    <td>reviews</td>
                    <td>DR</td>
                    <td>DR001, DR002, DR003, etc.</td>
                </tr>
                <tr>
                    <td>English</td>
                    <td>essays</td>
                    <td>EE</td>
                    <td>EE001, EE002, EE003, etc.</td>
                </tr>
                <tr>
                    <td>English</td>
                    <td>novels</td>
                    <td>EN</td>
                    <td>EN001, EN002, EN003, etc.</td>
                </tr>
                <tr>
                    <td>Greek</td>
                    <td>articles</td>
                    <td>GR</td>
                    <td>GR001, GR002, GR003, etc.</td>
                </tr>
                <tr>
                    <td>Spanish</td>
                    <td>articles</td>
                    <td>SP</td>
                    <td>SP001, SP002, SP003, etc.</td>
                </tr>
                </tbody>
            </table>
            <p>The ground truth data of the training corpus found in the file <code>truth.txt</code> include one
            line per problem with problem ID and the correct binary answer (Y means the known and the
            questioned documents are by the same author and N means the opposite). For example:</p>
            <pre class="prettyprint lang-py">EN001 N
EN002 Y
EN003 N
...
</pre>

            <h2 id="output">Output</h2>
            <p>Your software must take as input the absolute path to a set of problems. For each problem there
            is a separate sub-folder within that path including the set of known documents and the single
            unknown document of that problem (similarly to the training corpus). The software has to output
            a single text file <code>answers.txt</code> with all the produced answers for the whole set of
            evaluation problems. Each line of this file corresponds to a problem instance, it starts with
            the ID of the problem followed by a score, a real number in [0,1] inclusive, corresponding to
            the probability of a positive answer. That is, 0 means it is absolutely sure the questioned
            document is not by the author of the known documents, 1.0 means it is absolutely sure the
            questioned document and the known documents are by the same author, and 0.5 means that a
            positive and a negative answer are equally likely. The probability scores should be round with
            three decimal digits. Use a single whitespace to separate problem ID and probability score.<br/>
            For example, an <code>answers.txt</code> file may look like this:</p>
            <pre class="prettyprint lang-py" style="overflow-x:auto">EN001 0.031
EN002 0.874
EN003 0.500
...
</pre>


            <h2 id="evaluation">Evaluation</h2>
            <p>The participants’ answers will be evaluated according to the area under the ROC curve (AUC) of
            their probability scores.</p>
            <p>In addition, the performance of the binary classification results (automatically extracted from
            probability scores where every score greater than 0.5 corresponds to a positive answer, every
            score lower than 0.5 corresponds to a negative answer, while 0.5 corresponds to an unanswered
            problem, or an "I don’t know" answer) will be measured based on c@1
                (<a href="http://www.aclweb.org/anthology/P11-1142.pdf">Pe&ntilde;as &amp; Rodrigo, 2011</a>):
            </p>
            <ul>
                <li>c@1 = (1/<i>n</i>)*(<i>n</i><sub>c</sub>+(<i>n</i><sub>u</sub>*<i>n</i><sub>c</sub>/<i>n</i>))
                </li>
            </ul>
            <p>where:</p>
            <ul>
                <li><i>n</i> = #problems</li>
                <li><i>n</i><sub>c </sub>= #correct_answers</li>
                <li><i>n</i><sub>u </sub>= #unanswered_problems</li>
            </ul>
            <p><b>Note:</b> when positive/negative answers are provided for all available problems (probability
                scores different than 0.5), then c@1=accuracy. However, c@1 rewards approaches that maintain the
                same number of correct answers and decrease the number of incorrect answers by leaving some
                problems unanswered (when probability score equals 0.5).</p>
            <p>The final ranking of the participants will be based on the <b>product of AUC and c@1.</b></p>

            <h2 id="results">Results</h2>
            <table class="uk-table uk-table-divider uk-table-small uk-table-hover">
                <thead>
                <tr>
                    <th colspan="2" style="text-align:center">Authorship attribution performance</th>
                </tr>
                <tr>
                    <th>FinalScore</th>
                    <th>Team</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>0.566</td>
                    <td>Meta Classifier</td>
                </tr>
                <tr>
                    <td>0.490</td>
                    <td>Mahmoud Khonji and Youssef Iraqi<br/>Khalifa University, United Arab Emirates</td>
                </tr>
                <tr>
                    <td>0.484</td>
                    <td>Jordan Fréry°, Christine Largeron°, and Mihaela Juganaru-Mathieu*<br/>°Université de
                        Lyon and *École Nationale Supérieure des Mines, France
                    </td>
                </tr>
                <tr>
                    <td>0.461</td>
                    <td>Esteban Castillo°, Ofelia Cervantes°, Darnes Vilariño*, David Pinto*, and Saul
                        León*<br/>°Universidad de las Américas Puebla and *Benemérita Universidad Autónoma de
                        Puebla, Mexico
                    </td>
                </tr>
                <tr>
                    <td>0.451</td>
                    <td>Erwan Moreau, Arun Jayapal, and Carl Vogel<br/>Trinity College Dublin, Ireland</td>
                </tr>
                <tr>
                    <td>0.450</td>
                    <td>Cristhian Mayor, Josue Gutierrez, Angel Toledo, Rodrigo Martinez, Paola Ledesma, Gibran
                        Fuentes, and Ivan Meza<br/>Universidad Nacional Autonoma de Mexico, Mexico
                    </td>
                </tr>
                <tr>
                    <td>0.426</td>
                    <td>Hamed Zamani, Hossein Nasr, Pariya Babaie, Samira Abnar, Mostafa Dehghani, and Azadeh
                        Shakery<br/>University of Tehran, Iran
                    </td>
                </tr>
                <tr>
                    <td>0.400</td>
                    <td>Satyam, Anand, Arnav Kumar Dawn, and Sujan Kumar Saha<br/>Birla Institute of Technology,
                        India
                    </td>
                </tr>
                <tr>
                    <td>0.375</td>
                    <td>Pashutan Modaresi and Philipp Gross<br/>pressrelations GmbH, Germany</td>
                </tr>
                <tr>
                    <td>0.367</td>
                    <td>Magdalena Jankowska, Vlado Kešelj, and Evangelos Milios<br/>Dalhousie University, Canada
                    </td>
                </tr>
                <tr>
                    <td>0.335</td>
                    <td>Oren Halvani and Martin Steinebach<br/>Fraunhofer Institute for Secure Information
                        Technology SIT, Germany
                    </td>
                </tr>
                <tr>
                    <td>0.325</td>
                    <td>Baseline</td>
                </tr>
                <tr>
                    <td>0.308</td>
                    <td>Anna Vartapetiance and Lee Gillam<br/>University of Surrey, UK</td>
                </tr>
                <tr>
                    <td>0.306</td>
                    <td>Robert Layton<br/>Federation University, Australia</td>
                </tr>
                <tr>
                    <td>0.304</td>
                    <td>Sarah Harvey<br/>University of Waterloo, Canada</td>
                </tr>
                </tbody>
            </table>
            <p>A more detailed analysis of the detection performances can be found in the overview paper
                accompanying this task.</p>

            <h2 id="related-work">Related Work</h2>
            <ul>
                <li>
                    <a href="{{ 'publications.html#?q=2013%20juola' | relative_url }}">Author Identification, PAN @ CLEF'13</a>
                </li>
                <li>
                    <a href="{{ 'publications.html#?q=2012%20juola' | relative_url }}">Author Identification, PAN @ CLEF'12</a>
                </li>
                <li>
                    <a href="{{ 'publications.html#?q=2011%20argamon' | relative_url }}">Author Identification, PAN @ CLEF'11</a>
                </li>
                <li>
                    Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship Attribution</a>.
                    In Foundations and Trends in Information Retrieval, Volume 1, Issue 3, March 2008.
                </li>
                <li>
                    Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
                    <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">
                        Computational Methods Authorship Attribution</a>.
                    Journal of the American Society for Information Science and Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                </li>
                <li>
                    Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">
                        A Survey of Modern Authorship Attribution Methods</a>.
                    Journal of the American Society for Information Science and Technology, Volume 60, Issue 3, pages 538-556, March 2009.
                </li>
            </ul>


            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/stamatatos.html %}
                {% include people-cards/daelemans.html %}
                {% include people-cards/juola.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
                {% include people-cards/sanchez-perez.html %}
                {% include people-cards/verhoeven.html %}
                {% include people-cards/barron-cedeno.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2014 %}
            </div>

        </div>  <!-- section -->
    </div>  <!-- section -->
</main>
