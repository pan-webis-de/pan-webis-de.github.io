---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2019 - Cross-domain Authorship Attribution
description: PAN at CLEF 2019 - Cross-domain Authorship Attribution
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Cross-Domain Authorship Attribution 2019</a></li>
</ul>
</nav>

<main>
    <div class="uk-section uk-section-default">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Cross-Domain Authorship Attribution 2019</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#introduction">Introduction</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Development Phase</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#baselines">Baselines</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation Phase</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#output">Output</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#performance-measures">Performance Measures</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#submission">Submission</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>
        <div class="uk-container uk-margin-medium">
            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given a fanfiction text, determine its author among a list of candidates.</li>
                <li>Input: [<a href="{{ '../../data.html#pan19-authorship-attribution' | relative_url }}">data</a>]</li>
                <li>Evaluation: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef19/authorship-attribution" target="_blank">code</a>]</li>
                <li>Submission: [<a href="https://www.tira.io/task/authorship-attribution/">submit</a>]</li>
                <li>Baselines: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef19/authorship-attribution" target="_blank">code</a>]</li>
            </ul>
            <h2 id="introduction">Introduction</h2>
            <p>Authorship attribution is an important problem in information retrieval and computational
                linguistics
                but also in applied areas such as law and journalism where knowing the author of a document
                (such as
                a ransom note) may enable e.g. law enforcement to save lives. The most common framework for
                testing
                candidate algorithms is the closed-set attribution task: given a sample of reference documents
                from
                a restricted and finite set of candidate authors, the task is to determine the most likely
                author of
                a previously unseen document of unknown authorship. This task may be quite challenging in
                <strong>cross-domain
                    conditions</strong>, when documents of known and unknown authorship come from different
                domains
                (e.g., thematic area, genre). In addition, it is often more realistic to assume that the true
                author
                of a disputed document is not necessarily included in the list of candidates.</p>

            <p><strong>Fanfiction</strong> refers to fictional forms of literature which are nowadays produced
                by
                admirers ('fans') of a certain author (e.g. J.K. Rowling), novel ('Pride and Prejudice'), TV
                series
                (Sherlock Holmes), etc. The fans heavily borrow from the original work's theme, atmosphere,
                style,
                characters, story world etc. to produce new fictional literature, i.e. the so-called
                <strong>fanfics</strong>. This is why fanfiction is also known as transformative literature and
                has
                generated a number of controversies in recent years related to the intellectual rights property
                of
                the original authors (cf. plagiarism). Fanfiction, however, is typically produced by fans
                without
                any explicit commercial goals. The publication of fanfics typically happens online, on informal
                community platforms that are dedicated to making such literature accessible to a wider audience
                (e.g. <a href=https://www.fanfiction.net>fanfiction.net</a>). The original work of art or genre
                is
                typically refered to as a <strong>fandom</strong>.</p>
            <p>This edition of PAN focuses on cross-domain attribution in fanfiction, a task that can be more
                accurately described as <strong>cross-fandom attribution in fanfiction</strong>. In more detail,
                all
                documents of unknown authorship are fanfics of the same fandom (target fandom) while the
                documents
                of known authorship by the candidate authors are fanfics of several fandoms (other than the
                target-fandom). In contrast to the PAN-2018 edition of this task, we focus on <strong>open-set
                    attribution</strong> conditions, namely the true author of a text in the target domain is
                not
                necessarily included in the list of candidate authors.</p>

            <h2 id="task">Task</h2>
            <p>Given a set of documents (known fanfics) by a small number (up to 10) of candidate authors,
                identify
                the authors of another set of documents (unknown fanfics) in another target domain. Each
                candidate
                author has contributed at least one of the unknown fanfics, which all belong to the same target
                fandom. Some of the fanfics in the target domain were not written by any of the candidate
                authors.
                The known fanfics belong to several fandoms (excluding the target fandom), although not
                necessarily
                the same for all candidate authors. An equal number of known fanfics per candidate author is
                provided. In contrast, the unknown fanfics are not equally distributed over the authors. The
                text-length of fanfics varies from 500 to 1,000 tokens. All documents are in the same language
                that
                may be <strong>English, French, Italian, or Spanish</strong>.</p>

            <h2 id="data">Development Phase</h2>
            <p>To develop your software, we provide you with a corpus with highly
                similar characteristics to the evaluation corpus. It comprises a set of cross-domain authorship
                attribution problems in each of the following 5 languages: English, French, Italian, and
                Spanish. Note that we specifically avoid to use the term 'training corpus' because <strong>the
                    sets of candidate authors of the development and the evaluation corpora are not
                    overlapping</strong>. Therefore, your approach should not be designed to particularly handle
                the candidate authors of the development corpus. </p>
            <p>Each problem consists of a set of known fanfics by each candidate author and a set of unknown
                fanfics located in separate folders. The file <code>problem-info.json</code> that can be
                found in the main folder of each problem, shows the name of folder of unknown documents and
                the list of names of candidate author folders.</p>
            <pre class="prettyprint"><code class="lang-json">{
    "unknown-folder": "unknown",
    "candidate-authors": [
        { "author-name": "candidate00001" },
        { "author-name": "candidate00002" },
		...
	]
}</code></pre>
            <p>The fanfics of known authorship belong to several fandoms (excluding the target fandom). The
                file <code>fandom-info.json</code> (it can be found in the main folder of each problem)
                provides information about the fandom of all fanfics of known authorsihp, as follows. </p>
            <pre class="prettyprint"><code class="lang-json">[
    { "author-name": "candidate00001",
      "known-text": "known00001.txt",
      "fandom": "fandom00210"},
    { "author-name": "candidate00001",
      "known-text": "known00002.txt",
      "fandom": "fandom00051"},
    ...
]</code></pre>
            <p>The true author of each unknown document can be seen in the file
                <code>ground-truth.json</code>, also found in the main folder of each problem. Note that all
                unknown documents that are not written by any of the candidate authors belong to the <code>&lt;UNK&gt;</code>
                class.</p>
            <pre class="prettyprint"><code class="lang-json">{
    "ground_truth": [
        { "unknown-text": "unknown00001.txt",
          "true-author": "candidate00002" },
        { "unknown-text": "unknown00002.txt",
          "true-author": "&lt;UNK&gt;"},
        ...
    ]
}</code></pre>
            <p>In addition, to handle a collection of such problems, the file
                <code>collection-info.json</code> includes all relevant information. In more detail, for
                each problem it lists its main folder, the language (either <code>"en"</code>,
                <code>"fr"</code>, <code>"it"</code>, or <code>"sp"</code>), and the encoding (always <code>UTF-8</code>)
                of documents. </p>
            <pre class="prettyprint"><code class="lang-json">[
    { "problem-name": "problem00001",
      "language": "en",
      "encoding": "UTF-8" },
    { "problem-name": "problem00002",
      "language": "fr",
      "encoding": "UTF-8" },
 	  ...
]</code></pre>


            <h2 id="baselines">Baselines</h2>
            <p>We provide the implementation of baseline methods that can help you estimate the efficacy of
                your approach. It is also possible for you to start from a baseline method and attempt to
                improve it for that specific task. The following baselines are available:</p>
            <ul class="uk-list uk-list-bullet uk-list-large">
                <li>
                    <p><strong>BASELINE-SVM</strong>: This is a language-independent authorship attribution
                        approach based on a character 3-gram representation and a linear SVM classifier with
                        a reject option. It estimates the probabilities of output classes and assigns an
                        unknown document to the <code>&lt;UNK&gt;</code> class when the difference of the top
                        two
                        candidates is less than a threshold.</p>
                </li>
                <li><p><strong>BASELINE-COMPRESSOR</strong>: Another language-independent approach that uses
                    text compression to estimate the distance of an unknown document to each of the
                    candidate authors. It assigns an unknown document to the <code>&lt;UNK&gt;</code> class when
                    the
                    difference between the two most likely candidates is lower than a threshold.</p>
                <li><p><strong>BASELINE-IMPOSTERS</strong>: This baseline offers an implementation of the
                    language-independent "imposters" approach for authorship verification <a
                            href="https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.22954">(Koppel & Winter,
                        2014)</a>, based on character tetragram features. During a bootstrapped procedure, the
                    technique iteratively compares an unknown text to each candidate author's stylistic profile,
                    as well as to a set of imposter documents, on the basis of a random feature set. If the
                    highest ranking candidate author does not pass a fixed similarity threshold after this
                    procedure, the document is assigned to the <code>&lt;UNK&gt;</code> class and left
                    unattributed.</p>
                    <p>We also provide a set of imposter documents required by this baseline approach for each
                        of the four languages. This is a password-protected file (use the same password as for
                        the development corpus).</p>
                </li>
            </ul>

            <h2 id="evaluation">Evaluation Phase</h2>
            <p>Once you finished tuning your approach to achieve satisfying performance on
                the development corpus, your software will be tested on the evaluation corpus. During the
                competition, the evaluation corpus will not be released publicly. Instead, we ask you to
                <strong>submit your software</strong> for evaluation at our site as described below.</p>
            <p>After the competition, the evaluation corpus will become available including ground truth
                data. This way, you have all the necessities to evaluate your approach on your own, yet
                being comparable to those who took part in the competition. </p>

            <h2 id="output">Output</h2>
            <p>Your system should produce one output file for each authorship attribution problem in <a
                    href="http://www.json.org/">JSON</a>. The name of the output files should be <code>answers-PROBLEMNAME.json</code>
                (e.g., <code>answers-problem00001.json</code>, <code>answers-problem00002.json</code>)
                including the list of unknown documents and their predicted author:</p>

            <pre class="prettyprint"><code class="lang-json">[
    { "unknown-text":  "unknown00001.txt", 
      "predicted-author":  "candidate00003" },
    { "unknown-text":  "unknown00002.txt", 
      "predicted-author":  "&lt;UNK&gt;" },
	...
]</code></pre>

            <h2 id="performance-measures">Performance Measures</h2>
            <p>The submissions will be evaluated in each attribution problem separately based on their
                <strong>open-set macro-averaged F1 score</strong> (calculated over the training classes,
                that is when <code>&lt;UNK&gt;</code> is excluded) <a
                        href="https://link.springer.com/article/10.1007/s10994-016-5610-8">(Mendes et al.
                    2017)</a>. Participants will be ranked according to their average open-set macro-F1
                across all attribution problems of the evaluation corpus. </p>
            <p>We provide you with a Python script that calculates open-set macro-F1 for a collection of
                attribution problems
            </p>

            <h2 id="submission">Submission</h2>
            <p>We ask you to prepare your software so that it can be executed via command line calls. The
                command shall take as input (i) an absolute path to the directory of the evaluation corpus
                and (ii) an absolute path to an existing empty output directory:</p>
            <pre class="prettyprint"><code class="lang-cmd">mySoftware -i EVALUATION-DIRECTORY -o OUTPUT-DIRECTORY</code></pre>
            <p>Within <code>EVALUATION-DIRECTORY</code> a <code>collection-info.json</code> file and a
                number of folders, one for each attribution problem, will be found (similar to the
                development corpus as described above). For each attribution problem, the output file should
                be written in <code>OUTPUT-DIRECTORY</code>.</p>
            <p><strong>Note:</strong> Each attribution problem should be solved independently of other
                problems in the collection.</p>
            <p>You can choose freely among the available programming languages and among the operating
                systems Microsoft Windows and Ubuntu. We will ask you to deploy your software onto a virtual
                machine that will be made accessible to you after registration. You will be able to reach
                the virtual machine via ssh and via remote desktop. More information about how to access the
                virtual machines can be found in the user guide linked above.</p>
            <p>Once deployed in your virtual machine, we ask you to access TIRA at <a
                    href="http://www.tira.io">www.tira.io</a>, where you can self-evaluate your software on
                the test data.</p>
            <p><strong>Note:</strong> By submitting your software you retain full copyrights. You agree to
                grant us usage rights only for the purpose of the PAN competition. We agree not to share
                your software with a third party or use it for other purposes than the PAN competition.</p>

            <h2 id="related-work">Related Work</h2>
            <ul>
                <li>
                    <a href="http://ceur-ws.org/Vol-2125/invited_paper_2.pdf">Cross-domain Author identification
                        task at PAN@CLEF'18</a> (closed-set cross-domain authorship attribution in fanfiction)
                </li>
                <li>
                    Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                    Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                    March 2008.
                </li>
                <li>
                    Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                        href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                    Authorship Attribution</a>. Journal of the American Society for Information Science and
                    Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                </li>
                <li>
                    Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                    Survey of Modern Authorship Attribution Methods</a>.
                    Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                    pages 538-556, March 2009.
                </li>
            </ul>
            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/kestemont.html %}
                {% include people-cards/stamatatos.html %}
                {% include people-cards/manjavacas.html %}
                {% include people-cards/daelemans.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2019 %}
            </div>

        </div>  <!-- section -->
    </div>  <!-- section -->
</main>
