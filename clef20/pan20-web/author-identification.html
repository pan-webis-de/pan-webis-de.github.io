---
layout: default
nav_active: tasks
title: PAN @ CLEF 2020 - Authorship Verification
description: PAN @ CLEF 2020 - Authorship Verification
---

<main>
    <div class="uk-section uk-section-default">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Authorship Verification 2020</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task">Task</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#data">Data</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#evaluation">Evaluation</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#results">Results</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul>
                <li>Task: Given two texts, determine if they are written by the same author.</li>
                <li>Input: [<a href="{{ '../../data.html#pan20-authorship-verification' | relative_url }}">data</a>]</li>
                <li>Evaluation: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef20/authorship-verification">code</a>]</li>
                <li>Submission: [<a href="https://www.tira.io/task/authorship-verification/">submit</a>]</li>
                <li>Baseline: [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef20/authorship-verification">code</a>]</li>
            </ul>
            
            <h2 id="task">Task</h2>
            <p>Authorship verification is the task of deciding whether two texts have been written by the same author based on comparing the texts' writing styles.</p>
            <p>
              In the coming three years at PAN&nbsp;2020 to PAN&nbsp;2022, we develop a new experimental setup that addresses three key questions in authorship verification that have not been studied at scale to date:
              <ul>
              <li><p>Year 1 (PAN 2020): Closed-set verification.<br>Given a large training dataset comprising of known authors who have written about a given set of topics, the test dataset contains verification cases from a subset of the authors and topics found in the training data.</p></li>
              <li><p>Year 2 (PAN 2021): Open-set verification.<br>Given the training dataset of Year&nbsp;1, the test dataset contains verification cases from previously unseen authors and topics.</p></li>
              <li><p>Year 3 (PAN 2022): <em>Surprise task</em>.<br>The task of the last year of this evaluation cycle (to be announced at a later time) will be designed with an eye on realism and practical application.</p></li>
              </ul>
              This evaluation cycle on authorship verification provides for a renewed challenge of increasing difficulty within a large-scale evaluation. We invite you to plan ahead and participate in all three of these tasks.
            </p>
            
            
            <h2 id="data">Data</h2>
            <p>The train (calibration) and test datasets consists of pairs of (snippets from) two different fanfics, that were obtained drawn from <a href="https://www.fanfiction.net/">fanfiction.net</a>. Each pair was assigned a unique identifier and we distinguish between same-author pairs and different-authors pairs. Additionally, we offer metadata on the fandom (i.e. thematic category) for each text in the pair (note that fanfic "crossovers" were not included and only single-fandom texts were considered). The fandom distribution in these datasets maximally approximates the (long-tail) distribution of the fandoms in the original dataset. The test dataset is structured in the exact same way, but participants should expect a significant shift in the relation between authors and fandoms.</p>
            <p>The training dataset comes in two variants: a smaller dataset, particularly suited for symbolic machine learning methods and a large, dataset, suitable for applying data-hungry deep learning algorithms. Participants have to specify which of the two datasets was used to train their model. Models using the small set will be evaluated separately from models using the large set. We encourage participants to try the small dataset as a challenge, though participants can submit separate submissions for either one or both.</p>

            <p>Both the small and the large dataset come with two newline delimited JSON files each (<code>*.jsonl</code>).
            The first file contains pairs of texts (each pair has a unique ID) and their fandom labels:</p>

            <pre id="jsonl-example" class="prettyprint linenums uk-margin-remove-bottom">
<code class="lang-json">{"id": "6cced668-6e51-5212-873c-717f2bc91ce6", "fandoms": ["Fandom 1", "Fandom 2"], "pair": ["Text 1...", "Text 2..."]}
                        {"id": "ae9297e9-2ae5-5e3f-a2ab-ef7c322f2647", "fandoms": ["Fandom 3", "Fandom 4"], "pair": ["Text 3...", "Text 4..."]}
...</code></pre>

            <p>The second file, ending in <code>*_truth.jsonl</code>, contains the ground truth for all pairs. The ground truth
            is composed of a boolean flag indicating if texts in a pair are from the same author and the numeric author IDs:</p>
            <pre id="truth-jsonl-example" class="prettyprint linenums uk-margin-remove-bottom">
<code class="lang-json">{"id": "6cced668-6e51-5212-873c-717f2bc91ce6", "same": true, "authors": ["1446633", "1446633"]}
                        {"id": "ae9297e9-2ae5-5e3f-a2ab-ef7c322f2647", "same": false, "authors": ["1535385", "1998978"]}
...</code></pre>

            <p>Data and ground truth are in the same order and can be ingested line-wise in parallel without the need for a reshuffle based on the pair ID. The fandom labels will be given in both the training and testing datasets. The ground truth file will only be available for the training data.</p>

            <h2 id="baseline">Baselines [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef20/authorship-verification">code</a>]</h2>
            <p>We provide the following baseline methods: </p>
            <ul>
            <li>A simple method that calculates the cosine similarities between TFIDF-normalized, bag-of-character-tetragrams representations of the texts in a pair. The resulting scores are then shifted using a simple grid search, to arrive at an optimal performance on the calibration data. </li>
            <li>A simple method based on text compression that given a pair of texts calculates the cross-entropy of text2 using the Prediction by Partial Matching model of text1 and vice-versa. Then, the mean and absolute difference of the two cross-entropies are used by a logistic regression model to estimate a verification score in [0,1].</li>
            </ul>
            <p>Note that the above baseline methods do not make use of the fandom information: participants are highly encouraged to exploit this useful metadata in their submissions.</p>
            
            <h2 id="evaluation">Evaluation [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef20/authorship-verification">code</a>]</h2>
            <p>Systems will be compared and ranked on the basis of four, complementary metrics:
            <ul>
                <li><b>AUC</b>: the conventional area-under-the-curve score, as implemented in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html">scikit-learn</a>.</li>
                <li><b>F1-score</b>: the well-known performance measure (not taking into account non-answers), as implemented in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">scikit-learn</a>.</li>
                <li><b>c@1</b>: a variant of the conventional F1-score, which rewards systems that leave difficult problems unanswered (i.e. scores of exactly 0.5), introduced by Peñas and Rodrigo (2011).</li>
                <li><b>F_0.5u</b>: a newly proposed measure that puts more emphasis on deciding same-author cases correctly (Bevendorff et al. 2019)</li>
            </ul>

            <p>A reference evaluation script is made available. Systems will be applied to a pairs-file and are expected to produce a single jsonl-file as result. In this prediction file, each separate line should contain a valid json-string that provides the ID of a pair in the pairs-file and a "value" field, with a floating-point score that is bounded (0 >= score <= 1), indicating the probability that this pair is a same-author text pair. Systems are allowed to leave some problems unanswered: in such cases, the answer can be left out from the prediction file OR its value should be set to exactly 0.5. All answers that have a value of 0.5 will be considered non-decisions.</p>
            
            <h2 id="results">Results</h2>
            <p>More details will be shared soon.</p>
            
            <h2 id="related-work">Related Work</h2>
            <ul>
                <li>
                    <a href="http://ceur-ws.org/Vol-2125/invited_paper_2.pdf">Cross-domain Author identification
                    task at PAN@CLEF'18</a> (closed-set cross-domain authorship attribution in fanfiction)
                </li>
                <li>
                    Patrick Juola. <a href="http://portal.acm.org/citation.cfm?id=1373451">Authorship
                    Attribution</a>. In Foundations and Trends in Information Retrieval, Volume 1, Issue 3,
                    March 2008.
                </li>
                <li>
                    Moshe Koppel, Jonathan Schler, and Shlomo Argamon. <a
                        href="http://onlinelibrary.wiley.com/doi/10.1002/asi.20961/full">Computational Methods
                    Authorship Attribution</a>. Journal of the American Society for Information Science and
                    Technology, Volume 60, Issue 1, pages 9-26, January 2009.
                </li>
                <li>
                    Efstathios Stamatatos. <a href="http://onlinelibrary.wiley.com/doi/10.1002/asi.21001/full">A
                    Survey of Modern Authorship Attribution Methods</a>.
                    Journal of the American Society for Information Science and Technology, Volume 60, Issue 3,
                    pages 538-556, March 2009.
                </li>
                <li>
                    A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse. In Proc. of the 49th Annual Meeting of the Association for. Computational Linguistics, Vol. 1, pages 1415-1424, 2011.
                </li>
                <li>
                    Bevendorff et al. Generalizing Unmasking for Short Texts, Proceedings of NAACL (2019), 654-659.
                </li>
            </ul>

            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/kestemont.html %}
                {% include people-cards/stamatatos.html %}
                {% include people-cards/manjavacas.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2019 %}
            </div>
        </div>
    </div>
</main>
