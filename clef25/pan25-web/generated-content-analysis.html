---
layout: default
nav_active: shared-tasks
title: PAN at CLEF 2025 - Voight-Kampff Generative AI Detection
description: PAN at CLEF 2025 - Generative AI Detection
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="../../index.html">PAN</a></li>
<li><a href="../../shared-tasks.html">Shared Tasks</a></li>
<li class="uk-disabled"><a href="#">Generative AI Detection 2025</a></li>
</ul>
</nav>

<main class="uk-section uk-section-default">
    <div class="uk-container">
        <div class="uk-container uk-margin-small">
            <h1 class="uk-margin-remove-top">Voight-Kampff Generative AI Detection 2025</h1>
            <ul class="uk-list">
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#synopsis">Synopsis</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-overview">Task Overview</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task1">Subtask 1: Voight-Kampff AI Detection Sensitivity</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task2">Subtask 2: Human-AI Collaborative Text Classification</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#related-work">Related Work</a></li>
                <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#task-committee">Task Committee</a></li>
            </ul>
        </div>

        <div class="uk-container uk-margin-medium">

            <h2 id="synopsis">Synopsis</h2>
            <ul class="uk-list uk-list-bullet">
                <li>Subtask 1: Given a (potentially obfuscated) text, decide whether it was written by a human or an AI.</li>
                <li>Subtask 2: Given a document collaboratively authored by human and AI, classify the extent to which the model assisted.</li>
                <li>Important dates: <strong>May 23, 2025</strong> (software submission), <strong>May 30, 2025</strong> (participant notebook submission)</li>
                <li>Data: Human and machine texts [<a href="https://zenodo.org/records/14962653">download task 1</a>] [<a href="https://zenodo.org/records/14966981">download task 2</a>]</li>
                <li>Evaluation Measures: F1, C@1, AUC-ROC, FPR, FNR</li>
                <li>Baselines: SVM, Binoculars, RoBERTa [<a href="https://github.com/pan-webis-de/pan-code/tree/master/clef25/generative-authorship-verification/pan25_genai_baselines">code task 1</a>] [<a href="https://github.com/mbzuai-nlp/PAN-CLEF2025GenAIDetection-Subtask2">code task 2</a>]</li>
	        </ul>

            <h2 id="task-overview">Task Overview</h2>
            <p>The <em>Generative AI Authorship Verification Task</em> @ PAN is split into <strong>two subtasks</strong> [<a href="#task1">subtask 1</a>, <a href="#task2">subtask 2</a>]. Participants can submit their systems to either of them or both. Task 1 focuses on the robustness and sensitivity of detection systems, Task 2 focuses on the degree to which a mixed-authorship text is human- or machine-authored. The two tasks have individual datasets.</p>
        </div>
    </div>

        <!-- ======================== TASK 1 ======================== -->

    <div class="uk-section uk-section-default">
        <div class="uk-container uk-margin-medium">
            <h2 id="task1">Subtask 1: Voight-Kampff AI Detection Sensitivity</h2>
            <p>Subtask 1 is a binary AI detection task in that participants are given a text and have to decide whether it was machine-authored (class 1) or human-authored (class 0). However, we introduced a twist: The LLMs were instructed to change their style and mimic a specific human author. Furthermore, the test set will contain several surprises such as new models or unknown obfuscations to test the robustness of the classifiers (however, texts will be from the same domain).</p>

            <p>As in the previous year, the <em>Voight-Kampff AI detection Task</em> @ PAN is organized in collaboration with the <em>Voight-Kampff Task</em> @ <a href="https://eloquent-lab.github.io/">ELOQUENT Lab</a> Lab in a builder-breaker style.
            PAN participants will build systems to tell human and machine apart, while ELOQUENT participants will investigate novel text generation and obfuscation methods for avoiding detection.</p>

            <h3 id="data-task1">Data</h3>
            The dataset is available via <a href="https://zenodo.org/records/14962653">Zenodo</a>. Please register first at <a href="https://www.tira.io/task-overview/generative-ai-authorship-verification-panclef-2025/">Tira</a> and then request access on Zenodo using the same email address. The dataset contains copyrighted material and may be used only for research purposes. <strong>No redistribution allowed.</strong>

            <p>The training and validation dataset is provided as a set of newline-delimited JSON files. Each file contains a list of texts, written either by a human or a machine. The file format is as follows:</p>

            <pre class="prettyprint"><code class="lang-json">{"id": "a6c8018e-d22c-4d6e-b5e3-0c0a65682a6a", "text": "...", "model": "human", "label": 0, "genre": "essays"}
{"id": "f1a26761-ca2a-43e9-890d-80dcb3058364", "text": "...", "model": "gpt-4o", "label": 1, "genre": "essays"}
...</code></pre>

            A <code>"label"</code> of 0 means human-written, 1 is ai-written. <code>"genre"</code> is for informational purposes only and can be either <code>"essays"</code>, <code>"news"</code>, or <code>"fiction"</code>. Texts with <code>"genre": "news"</code> are sampled from last year's dataset (but with a few additions, such as GPT-4o). So if you want to reuse last year's dataset, be aware that some texts will be duplicates!

            The test dataset will have the same format, but with only the <code>"id"</code> and <code>"text"</code> columns.


            <h3 id="task1-submission">Submission</h3>
            <p>Participants will submit their systems as Docker images through the <a href="https://www.tira.io/task-overview/generative-ai-authorship-verification-panclef-2025/">Tira</a> platform. It is not expected that submitted systems are actually <em>trained</em> on Tira, but they must be standalone and runnable on the platform without requiring contact to the outside world (evaluation runs will be sandboxed).</p>

            <p>The submitted software must be executable inside the container via a command line call. The script must take two arguments: an input file (an absolute path to the input JSONL file) and an output directory (an absolute path to where the results will be written):</p>

            <p>Within Tira, the input file will be called <code>dataset.jsonl</code>, so with the pre-defined Tira placeholders, your software should be invoked like this:</p>
            <pre class="prettyprint"><code class="lang-console">$ mySoftware $inputDataset/dataset.jsonl $outputDir</code></pre>

            <p>Within <code>$outputDir</code>, a single (!) file with the file extension <code>*.jsonl</code> must be created with the following format:</p>

            <pre class="prettyprint"><code class="lang-json">{"id": "bea8cccd-0c99-4977-9c1b-8423a9e1ed96", "label": 1.0}
{"id": "a963d7a0-d7e9-47c0-be84-a40ccc2005c7", "label": 0.2315}
...</code></pre>

            <p>For each test case in the input file, an output line must be written with the ID of the input text pair and a confidence score between <code>0.0</code> and <code>1.0</code>. A score <code>&lt; 0.5</code> means that the text is believed to be human-authored. A score <code>&gt; 0.5</code> means that it is likely machine-written. A score of <em>exactly</em> <code>0.5</code> means the case is undecidable. Participants are encouraged to answer with <code>0.5</code> rather than making a <em>wrong</em> prediction. You can also give binary score (0 and 1) if your detector does not output class probabilities.</p>

            <p><strong>All test cases must be processed in isolation without information leakage between them!</strong> Even though systems may be given an input file with multiple JSON lines at once for reasons of efficiency, these inputs must be processed and answered just the same as if only a single line were given. Answers for any one test case must not depend on other cases in the input dataset!</p>

            <h3 id="task1-evaluation">Evaluation</h3>
            <p>Systems will be evaluated with the same measures as previous installments of the PAN authorship verification tasks. The following metrics will be used:</p>

            <ul class="uk-list uk-list-bullet">
                <li>ROC-AUC: The area under the ROC (Receiver Operating Characteristic) curve.</li>
                <li>Brier: The complement of the Brier score (mean squared loss).</li>
                <li>C@1: A modified accuracy score that assigns non-answers (score = 0.5) the average accuracy of the remaining cases.</li>
                <li>F<sub>1</sub>: The harmonic mean of precision and recall.</li>
                <li>F<sub>0.5u</sub>: A modified F<sub>0.5</sub> measure (precision-weighted F measure) that treats non-answers (score = 0.5) as false negatives.</li>
                <li>The arithmetic mean of all the metrics above.</li>
                <li>A confusion matrix for calculating true/false positive/negative rates.</li>
            </ul>

            <p>The evaluator for the task will output the above measures as JSON like so:</p>
            <pre class="prettyprint"><code class="lang-json">{
    "roc-auc": 0.996,
    "brier": 0.951,
    "c@1": 0.984,
    "f1": 0.98,
    "f05u": 0.981,
    "mean": 0.978,
    "confusion": [
        [
            1211,
            66
        ],
        [
            27,
            2285
        ]
    ]
}</code></pre>

            <h3 id="task1-baselines">Baselines</h3>
            <p>We provide three LLM detection baselines:</p>
            <ul class="uk-list uk-list-bullet">
                <li>
                    TF-IDF SVM
                </li><li>
                    PPMd Compression-based Cosine
                    [<a href="https://ieeexplore.ieee.org/abstract/document/1607268">Sculley and Brodley, 2006</a>]
                    [<a href="https://dl.acm.org/doi/abs/10.1145/3098954.3104050">Halvani et al., 2017</a>]
                </li><li>
                Binoculars
                [<a href="https://arxiv.org/abs/2401.12070">Hans et al., 2024</a>]
            </li>
            </ul>

            <p>With TF-IDF SVM and PPMd CBC, we provide two bag-of-words authorship verification models. Binoculars uses large language models to measure text perplexity. The baselines are published on <a href="https://github.com/pan-webis-de/pan-code/tree/master/clef25/generative-authorship-verification/pan25_genai_baselines">GitHub</a>. You can run them locally, in a Docker container or using <code>tira-run</code>. All baselines come with a CLI and usage instructions. Their general usage is:</p>

            <pre class="prettyprint"><code class="console">$ pan25-baseline BASELINENAME INPUT_FILE OUTPUT_DIRECTORY</code></pre>

            Use <code>--help</code> on any subcommand for more information:

            <pre class="prettyprint"><code class="console">$ pan25-baseline --help
    Usage: pan25-baseline [OPTIONS] COMMAND [ARGS]...

      PAN'25 Generative Authorship Detection baselines.

    Options:
      --help  Show this message and exit.

    Commands:
      binoculars  PAN'25 baseline: Binoculars.
      ppmd        PAN'25 baseline: Compression-based cosine.
      tfidf       PAN'25 baseline: TF-IDF SVM.</code></pre>

            <p>More information on how to install and run the baselines can be found in the <a href="https://github.com/pan-webis-de/pan-code/tree/master/clef25/generative-authorship-verification/pan25_genai_baselines">README on GitHub</a>.</p>

        </div>
    </div>


        <!-- ======================== TASK 2 ======================== -->

    <div class="uk-section uk-section-muted">
        <div class="uk-container uk-margin-medium">
            <h2 id="task2">Subtask 2: Human-AI Collaborative Text Classification</h2>
            <p>In subtask 2, we focus on Human-AI Collaborative Text Classification, where the goal is to categorize documents that have been co-authored by humans and LLMs. Specifically, we aim to classify texts into six distinct categories based on the nature of human and machine contributions:</p>

            <ol class="uk-list uk-list-bullet">
                <li><strong>Fully human-written:</strong> The document is entirely authored by a human without any AI assistance.</li>
                <li><strong>Human-initiated, then machine-continued:</strong> A human starts writing, and an AI model completes the text.</li>
                <li><strong>Human-written, then machine-polished:</strong> The text is initially written by a human but later refined or edited by an AI model.</li>
                <li><strong>Machine-written, then machine-humanized (obfuscated):</strong> An AI generates the text, which is later modified to obscure its machine origin.</li>
                <li><strong>Machine-written, then human-edited:</strong> The content is generated by an AI but subsequently edited or refined by a human.</li>
                <li><strong>Deeply-mixed text:</strong> The document contains interwoven sections written by both humans and AI, without a clear separation.</li>
            </ol>

            <p>Accurately distinguishing between these categories will enhance our understanding of human-AI collaboration and help mitigate the risks associated with synthetic text.</p>

            <h3 id="task2-data">Data</h3>
            The dataset for Task 2 can be downloaded from <a href="https://zenodo.org/records/14966981">Zenodo</a>. More information and baselines can be found in our <a href="https://github.com/mbzuai-nlp/PAN-CLEF2025GenAIDetection-Subtask2">GitHub repository</a>.

            <ul class="uk-list uk-list-bullet">
                <li>Multi-domain documents (academic, journalism, social media)</li>
                <li>Human-written and machine-generated samples (GPT-4, Claude, PaLM)</li>
                <li>Collaborative texts with annotation layers for human/machine contributions</li>
                <li>Multiple languages supported (English, Spanish, German)</li>
            </ul>

            <p>Dataset label distribution:</p>

            <table class="uk-table uk-table-divider uk-table-small uk-width-1-2">
                <thead>
                <tr>
                    <th>Label Category</th>
                    <th class="uk-text-right">Train</th>
                    <th class="uk-text-right">Dev</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Machine-written, then machine-humanized</td>
                    <td class="uk-text-right">91,232</td>
                    <td class="uk-text-right">10,137</td>
                </tr>
                <tr>
                    <td>Human-written, then machine-polished</td>
                    <td class="uk-text-right">95,398</td>
                    <td class="uk-text-right">12,289</td>
                </tr>
                <tr>
                    <td>Fully human-written</td>
                    <td class="uk-text-right">75,270</td>
                    <td class="uk-text-right">12,330</td>
                </tr>
                <tr>
                    <td>Human-initiated, then machine-continued</td>
                    <td class="uk-text-right">10,740</td>
                    <td class="uk-text-right">37,170</td>
                </tr>
                <tr>
                    <td>Deeply-mixed text (human + machine parts)</td>
                    <td class="uk-text-right">14,910</td>
                    <td class="uk-text-right">225</td>
                </tr>
                <tr>
                    <td>Machine-written, then human-edited</td>
                    <td class="uk-text-right">1,368</td>
                    <td class="uk-text-right">510</td>
                </tr>
                <tr>
                    <td><strong>Total</strong></td>
                    <td class="uk-text-right"><strong>288,918</strong></td>
                    <td class="uk-text-right"><strong>72,661</strong></td>
                </tr>
                </tbody>
            </table>

            <h3 id="task2-submission">Submission</h3>
           	<p>Participants only need to submit the predicted labels by a jsonl file named as<code>team_name_submission_date.jsonl</code> to <a href="https://codalab.lisn.upsaclay.fr/competitions/22620">CodaLab</a>.</p>
		<p>The <code>team_name_submission_date.jsonl</code> file should be a lines of objects by the following format:</p>
		 <pre class="prettyprint"><code class="lang-json">{"id": "identifier of the test sample", "label": 1} 
{"id": "identifier of the test sample", "label": 3}
...</code></pre>
		<p>labels are int from 0-5, that is, [0, 1, 2, 3, 4, 5]. Here is mapping.</p>   
<pre class="prettyprint"><code class="lang-json">id2label = {
0: “fully human-written”, 
1: “human-written, then machine-polished”, 
2: “machine-written, then machine-humanized”, 
3: “human-initiated, then machine-continued”,
4: “deeply-mixed text; where some parts are written by a human and some are generated by a machine”, 
5: “machine-written, then human-edited”
}</code></pre>
		<p>Check the correctness of your submission format by <a href="https://github.com/mbzuai-nlp/PAN-CLEF2025GenAIDetection-Subtask2/blob/main/format_checker.py">format_checker.py</a> in our <a href="https://github.com/mbzuai-nlp/PAN-CLEF2025GenAIDetection-Subtask2">GitHub repository</a>. This script checks whether the results format is correct. It also provides some warnings about possible errors.</p>

            <h3 id="task2-evaluation">Evaluation</h3>
            <p>tba</p>

            <h3 id="task2-baselines">Baselines</h3>
            <ul class="uk-list uk-list-bullet">
                <li>Zero-shot detectors: DetectGPT, Binoculars</li>
                <li>Fine-tuned LLMs: RoBERTa-base, DeBERTa-v3</li>
                <li>Ensemble methods with stylometric features</li>
            </ul>
            <p>Code for the task baseline can be found in our <a href="https://github.com/mbzuai-nlp/PAN-CLEF2025GenAIDetection-Subtask2">GitHub repository</a>.</p>

        </div>
    </div>


    <div class="uk-section uk-section-default">
        <div class="uk-container uk-margin-medium">
            <h2 id="related-work">Related Work</h2>
            <ul class="uk-list uk-list-bullet">
				<li>
                    Janek Bevendorff, Matti Wiegmann, Jussi Karlgren, Luise Dürlich, Evangelia Gogoulou, Aarne Talman, Efstathios Stamatatos, Martin Potthast, and Benno Stein. <a href="https://downloads.webis.de/publications/papers/bevendorff_2024d.pdf">Overview of the “Voight-Kampff” Generative AI Authorship Verification Task at PAN and ELOQUENT 2024.</a> In Guglielmo Faggioli, Nicola Ferro, Petra Galuščáková, and Alba García Seco de Herrera, editors, Working Notes of CLEF 2024 – Conference and Labs of the Evaluation Forum, CEUR Workshop Proceedings, pages 2486-2506, September 2024. CEUR-WS.org.
                </li>
				<li>
                    Bevendorff, Janek, Xavier Bonet Casals, Berta Chulvi, Daryna Dementieva, Ashaf Elnagar, Dayne Freitag, Maik Fröbe, et al. 2024. <a href="https://webis.de/publications.html#bevendorff_2024b">Overview of PAN 2024: Multi-Author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification: Extended Abstract.</a> In Lecture Notes in Computer Science, 3-10. Lecture Notes in Computer Science. Cham: Springer Nature Switzerland.
                </li>
                <li>
                    Uchendu, Adaku, Thai Le, Kai Shu, and Dongwon Lee. 2020. <a href="https://aclanthology.org/2020.emnlp-main.673/">Authorship Attribution for Neural Text Generation.</a> In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 8384-95. Online: Association for Computational Linguistics.
                </li>
                <li>
                    Jakesch, Maurice, Jeffrey T. Hancock, and Mor Naaman. 2023. <a href="https://www.pnas.org/doi/10.1073/pnas.2208839120">Human Heuristics for AI-Generated Language Are Flawed.</a> Proceedings of the National Academy of Sciences of the United States of America 120 (11): e2208839120.
                </li>
                <li>
                    Hans, Abhimanyu, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. <a href="http://arxiv.org/abs/2401.12070">Spotting LLMs with Binoculars: Zero-Shot Detection of Machine-Generated Text.</a> arXiv [Cs.CL].
                </li>
                <li>
                    Su, Jinyan, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. <a href="https://arxiv.org/abs/2306.05540">DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text.</a> arXiv [Cs.CL].
                </li>
                <li>
                    Mitchell, Eric, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. <a href="http://arxiv.org/abs/2301.11305">DetectGPT: Zero-Shot Machine-Generated Text Detection Using Probability Curvature.</a> arXiv [Cs.CL].
                </li>
                <li>
                    Bao, Guangsheng, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2023. <a href="https://arxiv.org/abs/2310.05130">Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature.</a> arXiv [Cs.CL].
                </li>
                <li>
                    Koppel, Moshe, and Jonathan Schler. 2004. <a href="https://dl.acm.org/doi/abs/10.1145/1015330.1015448">Authorship Verification as a One-Class Classification Problem.</a> In Proceedings, Twenty-First International Conference on Machine Learning, ICML 2004, 489-95.
                </li>
                <li>
                    Bevendorff, Janek, Benno Stein, Matthias Hagen, and Martin Potthast. 2019. <a href="https://webis.de/publications.html#bevendorff_2019a">Generalizing Unmasking for Short Texts.</a> In Proceedings of the 2019 Conference of the North, 654-59. Stroudsburg, PA, USA: Association for Computational Linguistics.
                </li>
                <li>
                    Sculley, D., and C. E. Brodley. 2006. <a href="https://ieeexplore.ieee.org/abstract/document/1607268">Compression and Machine Learning: A New Perspective on Feature Space Vectors.</a> In Data Compression Conference (DCC'06), 332-41. IEEE.
                </li>
                <li>
                    Halvani, Oren, Christian Winter, and Lukas Graner. 2017. <a href="https://dl.acm.org/doi/abs/10.1145/3098954.3104050">On the Usefulness of Compression Models for Authorship Verification.</a> In ACM International Conference Proceeding Series. Vol. Part F1305. Association for Computing Machinery. https://doi.org/10.1145/3098954.3104050.
                </li>
                <li>
                    Uchendu, Adaku, Zeyu Ma, Thai Le, Rui Zhang, and Dongwon Lee. 2021. <a href="https://aclanthology.org/2021.findings-emnlp.172/">TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation.</a> In Findings of the Association for Computational Linguistics: EMNLP 2021, 2001-16. Stroudsburg, PA, USA: Association for Computational Linguistics.
                </li>
                <li>
                    Schuster, Tal, Roei Schuster, Darsh J. Shah, and Regina Barzilay. 2020. <a href="https://direct.mit.edu/coli/article/46/2/499/93369/The-Limitations-of-Stylometry-for-Detecting">The Limitations of Stylometry for Detecting Machine-Generated Fake News.</a> Computational Linguistics 46 (2): 499-510.
                </li>
                <li>
                    Sadasivan, Vinu Sankar, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. <a href="http://arxiv.org/abs/2303.11156">Can AI-Generated Text Be Reliably Detected?</a> arXiv [Cs.CL].
                </li>
                <li>
                    Ippolito, Daphne, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. <a href="https://aclanthology.org/2020.acl-main.164/">Automatic Detection of Generated Text Is Easiest When Humans Are Fooled.</a> In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 1808-22. Stroudsburg, PA, USA: Association for Computational Linguistics.
                </li>
            </ul>

            <h2 id="task-committee">Task Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/bevendorff.html %}
                {% include people-cards/y_wang.html %}
                {% include people-cards/wiegmann.html %}
                {% include people-cards/shelmanov.html %}
                {% include people-cards/mansurov.html %}
                {% include people-cards/tsvigun.html %}
                {% include people-cards/gurevych.html %}
                {% include people-cards/nakov.html %}
                {% include people-cards/stamatatos.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/stein.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2025 ows=true %}
            </div>
        </div>
    </div>
</main>
